{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaIsq2Zh08wL"
      },
      "source": [
        "# Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PEDd77kx08wN"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import ssl\n",
        "from keras.utils import to_categorical\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_xB7vkv_08wO"
      },
      "outputs": [],
      "source": [
        "# loading in the MNIST dataset for Q1\n",
        "# Obtaining the CIFAR-10 dataset for the implementation for both training and testing data using built-in functions\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_train = x_train.astype('float32') / 255\n",
        "testX = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# target column\n",
        "y_test = to_categorical(y_test)\n",
        "y_train = to_categorical(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models that goes through the iteration for so long that I printed out the model accuracy so you can see that it runs without having to run it yourself. If you want, you can do that as well. It takes approximately 45 mins for each."
      ],
      "metadata": {
        "id": "eUCFIFGCgwFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4SInbCIfu9FR"
      },
      "outputs": [],
      "source": [
        "# iterating parameters for all the models\n",
        "learning_rate = [0.001, 0.01, 0.1]\n",
        "batch_size = [25, 50, 100]\n",
        "optimizer_choice = [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9-2cr1Dm69T",
        "outputId": "4c9e934d-1b52-4e7b-f8a2-444089e3f17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 17s 6ms/step - loss: 0.2244 - accuracy: 0.9302 - val_loss: 0.0881 - val_accuracy: 0.9719\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0739 - accuracy: 0.9775 - val_loss: 0.0502 - val_accuracy: 0.9841\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0583 - accuracy: 0.9822 - val_loss: 0.0432 - val_accuracy: 0.9862\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0474 - accuracy: 0.9858 - val_loss: 0.0566 - val_accuracy: 0.9833\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.0597 - val_accuracy: 0.9844\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0363 - accuracy: 0.9887 - val_loss: 0.0453 - val_accuracy: 0.9865\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0326 - accuracy: 0.9898 - val_loss: 0.0438 - val_accuracy: 0.9874\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0291 - accuracy: 0.9909 - val_loss: 0.0379 - val_accuracy: 0.9889\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0268 - accuracy: 0.9918 - val_loss: 0.0564 - val_accuracy: 0.9839\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.0579 - val_accuracy: 0.9851\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.0579 - accuracy: 0.9851\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 0.7247 - accuracy: 0.7633 - val_loss: 0.1974 - val_accuracy: 0.9398\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.1927 - accuracy: 0.9427 - val_loss: 0.1349 - val_accuracy: 0.9582\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.1355 - accuracy: 0.9589 - val_loss: 0.1215 - val_accuracy: 0.9619\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.1088 - accuracy: 0.9670 - val_loss: 0.0899 - val_accuracy: 0.9708\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0933 - accuracy: 0.9718 - val_loss: 0.0763 - val_accuracy: 0.9761\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0804 - accuracy: 0.9754 - val_loss: 0.0668 - val_accuracy: 0.9787\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0726 - accuracy: 0.9775 - val_loss: 0.0766 - val_accuracy: 0.9758\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0656 - accuracy: 0.9794 - val_loss: 0.0616 - val_accuracy: 0.9814\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0591 - accuracy: 0.9811 - val_loss: 0.0671 - val_accuracy: 0.9791\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0572 - val_accuracy: 0.9816\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0572 - accuracy: 0.9816\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1883 - accuracy: 0.9437 - val_loss: 0.0857 - val_accuracy: 0.9783\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.0848 - accuracy: 0.9771 - val_loss: 0.0670 - val_accuracy: 0.9814\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0845 - accuracy: 0.9789 - val_loss: 0.0630 - val_accuracy: 0.9812\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.0989 - accuracy: 0.9778 - val_loss: 0.1032 - val_accuracy: 0.9801\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.1233 - accuracy: 0.9749 - val_loss: 0.0763 - val_accuracy: 0.9762\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.1346 - accuracy: 0.9731 - val_loss: 0.1452 - val_accuracy: 0.9796\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.1504 - accuracy: 0.9712 - val_loss: 0.0822 - val_accuracy: 0.9822\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.1754 - accuracy: 0.9683 - val_loss: 0.1401 - val_accuracy: 0.9752\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1857 - accuracy: 0.9678 - val_loss: 0.1288 - val_accuracy: 0.9730\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 0.1962 - accuracy: 0.9658 - val_loss: 0.1520 - val_accuracy: 0.9540\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.9540\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 9s 6ms/step - loss: 0.1774 - accuracy: 0.9454 - val_loss: 0.0674 - val_accuracy: 0.9792\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0657 - accuracy: 0.9796 - val_loss: 0.0639 - val_accuracy: 0.9817\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0513 - accuracy: 0.9845 - val_loss: 0.0454 - val_accuracy: 0.9865\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0426 - accuracy: 0.9868 - val_loss: 0.0556 - val_accuracy: 0.9824\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0366 - accuracy: 0.9888 - val_loss: 0.0414 - val_accuracy: 0.9860\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0319 - accuracy: 0.9899 - val_loss: 0.0428 - val_accuracy: 0.9872\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0291 - accuracy: 0.9910 - val_loss: 0.0494 - val_accuracy: 0.9867\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0411 - val_accuracy: 0.9882\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0231 - accuracy: 0.9926 - val_loss: 0.0413 - val_accuracy: 0.9896\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 0.0320 - val_accuracy: 0.9904\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0320 - accuracy: 0.9904\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 1.2617 - accuracy: 0.5589 - val_loss: 0.4207 - val_accuracy: 0.8732\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.3654 - accuracy: 0.8858 - val_loss: 0.2564 - val_accuracy: 0.9222\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.2370 - accuracy: 0.9273 - val_loss: 0.1972 - val_accuracy: 0.9408\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1838 - accuracy: 0.9432 - val_loss: 0.1504 - val_accuracy: 0.9533\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1527 - accuracy: 0.9529 - val_loss: 0.1463 - val_accuracy: 0.9563\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1325 - accuracy: 0.9587 - val_loss: 0.1236 - val_accuracy: 0.9614\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.1178 - accuracy: 0.9632 - val_loss: 0.1091 - val_accuracy: 0.9645\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1077 - accuracy: 0.9661 - val_loss: 0.0951 - val_accuracy: 0.9695\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0970 - accuracy: 0.9704 - val_loss: 0.0993 - val_accuracy: 0.9675\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0906 - accuracy: 0.9715 - val_loss: 0.0844 - val_accuracy: 0.9728\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0844 - accuracy: 0.9728\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 11s 8ms/step - loss: 0.2010 - accuracy: 0.9391 - val_loss: 0.0586 - val_accuracy: 0.9816\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0695 - accuracy: 0.9797 - val_loss: 0.0487 - val_accuracy: 0.9855\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0569 - accuracy: 0.9841 - val_loss: 0.0500 - val_accuracy: 0.9864\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0534 - accuracy: 0.9855 - val_loss: 0.0384 - val_accuracy: 0.9888\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0506 - accuracy: 0.9869 - val_loss: 0.0712 - val_accuracy: 0.9790\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0531 - accuracy: 0.9871 - val_loss: 0.0478 - val_accuracy: 0.9868\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0587 - accuracy: 0.9858 - val_loss: 0.1193 - val_accuracy: 0.9795\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0654 - accuracy: 0.9857 - val_loss: 0.0810 - val_accuracy: 0.9842\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0705 - accuracy: 0.9851 - val_loss: 0.0546 - val_accuracy: 0.9898\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0716 - accuracy: 0.9854 - val_loss: 0.0643 - val_accuracy: 0.9850\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0643 - accuracy: 0.9850\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 7s 9ms/step - loss: 0.1830 - accuracy: 0.9423 - val_loss: 0.0754 - val_accuracy: 0.9759\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.0550 - val_accuracy: 0.9847\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0494 - accuracy: 0.9849 - val_loss: 0.0442 - val_accuracy: 0.9864\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0402 - accuracy: 0.9872 - val_loss: 0.0416 - val_accuracy: 0.9872\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0384 - val_accuracy: 0.9887\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.0411 - val_accuracy: 0.9883\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0279 - accuracy: 0.9915 - val_loss: 0.0349 - val_accuracy: 0.9888\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.0371 - val_accuracy: 0.9883\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0231 - accuracy: 0.9926 - val_loss: 0.0316 - val_accuracy: 0.9908\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0332 - val_accuracy: 0.9914\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0332 - accuracy: 0.9914\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1.2722 - accuracy: 0.5744 - val_loss: 0.6032 - val_accuracy: 0.8096\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.5302 - accuracy: 0.8304 - val_loss: 0.4293 - val_accuracy: 0.8577\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.3996 - accuracy: 0.8721 - val_loss: 0.3460 - val_accuracy: 0.8875\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.3334 - accuracy: 0.8935 - val_loss: 0.3108 - val_accuracy: 0.8963\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.2907 - accuracy: 0.9071 - val_loss: 0.2565 - val_accuracy: 0.9166\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.2626 - accuracy: 0.9159 - val_loss: 0.2435 - val_accuracy: 0.9225\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.2406 - accuracy: 0.9237 - val_loss: 0.2359 - val_accuracy: 0.9219\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.2217 - accuracy: 0.9291 - val_loss: 0.2113 - val_accuracy: 0.9299\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.2072 - accuracy: 0.9345 - val_loss: 0.2022 - val_accuracy: 0.9371\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.1940 - accuracy: 0.9379 - val_loss: 0.1962 - val_accuracy: 0.9372\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.1962 - accuracy: 0.9372\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 8s 10ms/step - loss: 0.2930 - accuracy: 0.9087 - val_loss: 0.1655 - val_accuracy: 0.9452\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0761 - accuracy: 0.9771 - val_loss: 0.0718 - val_accuracy: 0.9773\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0545 - accuracy: 0.9836 - val_loss: 0.0437 - val_accuracy: 0.9853\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.0366 - val_accuracy: 0.9890\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0367 - accuracy: 0.9890 - val_loss: 0.0345 - val_accuracy: 0.9903\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0328 - accuracy: 0.9899 - val_loss: 0.0399 - val_accuracy: 0.9881\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0272 - accuracy: 0.9926 - val_loss: 0.0445 - val_accuracy: 0.9896\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0263 - accuracy: 0.9922 - val_loss: 0.0439 - val_accuracy: 0.9899\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0244 - accuracy: 0.9929 - val_loss: 0.0440 - val_accuracy: 0.9872\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0246 - accuracy: 0.9931 - val_loss: 0.0429 - val_accuracy: 0.9899\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.9899\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 0.4269 - accuracy: 0.8696 - val_loss: 0.2288 - val_accuracy: 0.9252\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 0.2759 - accuracy: 0.9169 - val_loss: 0.2916 - val_accuracy: 0.9105\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2477 - accuracy: 0.9264 - val_loss: 0.2599 - val_accuracy: 0.9215\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2457 - accuracy: 0.9282 - val_loss: 0.1928 - val_accuracy: 0.9421\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 0.2535 - accuracy: 0.9257 - val_loss: 0.2310 - val_accuracy: 0.9360\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2364 - accuracy: 0.9345 - val_loss: 0.5217 - val_accuracy: 0.8649\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2280 - accuracy: 0.9337 - val_loss: 0.1940 - val_accuracy: 0.9463\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2163 - accuracy: 0.9362 - val_loss: 0.2129 - val_accuracy: 0.9414\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2309 - accuracy: 0.9326 - val_loss: 0.1901 - val_accuracy: 0.9430\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.2132 - accuracy: 0.9380 - val_loss: 0.1692 - val_accuracy: 0.9522\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1692 - accuracy: 0.9522\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 0.2682 - accuracy: 0.9144 - val_loss: 0.0944 - val_accuracy: 0.9694\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0799 - accuracy: 0.9753 - val_loss: 0.1321 - val_accuracy: 0.9586\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0582 - accuracy: 0.9817 - val_loss: 0.0493 - val_accuracy: 0.9841\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0452 - accuracy: 0.9860 - val_loss: 0.0454 - val_accuracy: 0.9853\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0357 - accuracy: 0.9886 - val_loss: 0.0458 - val_accuracy: 0.9842\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.0401 - val_accuracy: 0.9868\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0254 - accuracy: 0.9914 - val_loss: 0.0362 - val_accuracy: 0.9883\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0444 - val_accuracy: 0.9873\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.0384 - val_accuracy: 0.9882\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.0429 - val_accuracy: 0.9873\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.9873\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 19s 7ms/step - loss: 2.4318 - accuracy: 0.1084 - val_loss: 2.3029 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3029 - accuracy: 0.1097 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3024 - accuracy: 0.1093 - val_loss: 2.3027 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3029 - accuracy: 0.1098 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3028 - accuracy: 0.1096 - val_loss: 2.3026 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3028 - accuracy: 0.1090 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3027 - accuracy: 0.1095 - val_loss: 2.3027 - val_accuracy: 0.1028\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3026 - accuracy: 0.1098 - val_loss: 2.3024 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3027 - accuracy: 0.1102 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3026 - accuracy: 0.1096 - val_loss: 2.3020 - val_accuracy: 0.1010\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3020 - accuracy: 0.1010\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.4193 - accuracy: 0.8644 - val_loss: 0.2108 - val_accuracy: 0.9322\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.2053 - accuracy: 0.9380 - val_loss: 0.1846 - val_accuracy: 0.9382\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.1761 - accuracy: 0.9464 - val_loss: 0.1446 - val_accuracy: 0.9535\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1746 - accuracy: 0.9481 - val_loss: 0.1597 - val_accuracy: 0.9542\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.1658 - accuracy: 0.9515 - val_loss: 0.1896 - val_accuracy: 0.9431\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1741 - accuracy: 0.9492 - val_loss: 0.1375 - val_accuracy: 0.9564\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1614 - accuracy: 0.9522 - val_loss: 0.2353 - val_accuracy: 0.9313\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1558 - accuracy: 0.9543 - val_loss: 0.1509 - val_accuracy: 0.9570\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1594 - accuracy: 0.9522 - val_loss: 0.1482 - val_accuracy: 0.9569\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1995 - accuracy: 0.9409 - val_loss: 0.1327 - val_accuracy: 0.9613\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1327 - accuracy: 0.9613\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.3791 - accuracy: 0.8792 - val_loss: 0.1001 - val_accuracy: 0.9684\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.1007 - accuracy: 0.9692 - val_loss: 0.0730 - val_accuracy: 0.9774\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.0709 - accuracy: 0.9777 - val_loss: 0.0558 - val_accuracy: 0.9811\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0554 - accuracy: 0.9830 - val_loss: 0.0575 - val_accuracy: 0.9815\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0462 - accuracy: 0.9851 - val_loss: 0.0513 - val_accuracy: 0.9831\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0387 - accuracy: 0.9876 - val_loss: 0.0591 - val_accuracy: 0.9810\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0339 - accuracy: 0.9891 - val_loss: 0.0439 - val_accuracy: 0.9863\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0267 - accuracy: 0.9914 - val_loss: 0.0561 - val_accuracy: 0.9840\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0432 - val_accuracy: 0.9869\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0203 - accuracy: 0.9933 - val_loss: 0.0420 - val_accuracy: 0.9879\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0420 - accuracy: 0.9879\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 10s 8ms/step - loss: 2.7149 - accuracy: 0.1092 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 9s 8ms/step - loss: 2.3022 - accuracy: 0.1098 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3023 - accuracy: 0.1106 - val_loss: 2.3021 - val_accuracy: 0.1009\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3021 - accuracy: 0.1105 - val_loss: 2.3027 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3021 - accuracy: 0.1113 - val_loss: 2.3024 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3023 - accuracy: 0.1095 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3024 - accuracy: 0.1103 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3023 - accuracy: 0.1094 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3022 - accuracy: 0.1112 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3024 - accuracy: 0.1108 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3012 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.5922 - accuracy: 0.8175 - val_loss: 0.1959 - val_accuracy: 0.9383\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.1865 - accuracy: 0.9427 - val_loss: 0.1491 - val_accuracy: 0.9527\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.1560 - accuracy: 0.9528 - val_loss: 0.1259 - val_accuracy: 0.9607\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.1353 - accuracy: 0.9588 - val_loss: 0.1086 - val_accuracy: 0.9658\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.1324 - accuracy: 0.9598 - val_loss: 0.1072 - val_accuracy: 0.9681\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.1321 - accuracy: 0.9598 - val_loss: 0.1289 - val_accuracy: 0.9628\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.1188 - accuracy: 0.9637 - val_loss: 0.1244 - val_accuracy: 0.9630\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.1134 - accuracy: 0.9657 - val_loss: 0.1059 - val_accuracy: 0.9699\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.1102 - accuracy: 0.9667 - val_loss: 0.0948 - val_accuracy: 0.9693\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.1115 - accuracy: 0.9666 - val_loss: 0.1036 - val_accuracy: 0.9687\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1036 - accuracy: 0.9687\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.5002 - accuracy: 0.8384 - val_loss: 0.1284 - val_accuracy: 0.9616\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.1262 - accuracy: 0.9609 - val_loss: 0.0863 - val_accuracy: 0.9720\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0877 - accuracy: 0.9730 - val_loss: 0.0715 - val_accuracy: 0.9767\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0695 - accuracy: 0.9783 - val_loss: 0.0677 - val_accuracy: 0.9789\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0578 - accuracy: 0.9822 - val_loss: 0.0640 - val_accuracy: 0.9791\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0504 - accuracy: 0.9836 - val_loss: 0.0546 - val_accuracy: 0.9820\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0441 - accuracy: 0.9865 - val_loss: 0.0519 - val_accuracy: 0.9838\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0401 - accuracy: 0.9877 - val_loss: 0.0509 - val_accuracy: 0.9832\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0354 - accuracy: 0.9886 - val_loss: 0.0526 - val_accuracy: 0.9828\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 0.0317 - accuracy: 0.9901 - val_loss: 0.0464 - val_accuracy: 0.9853\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0464 - accuracy: 0.9853\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 7s 10ms/step - loss: 6.2926 - accuracy: 0.1111 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3020 - accuracy: 0.1116 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3020 - accuracy: 0.1114 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3018 - accuracy: 0.1106 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3020 - accuracy: 0.1100 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3020 - accuracy: 0.1109 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3019 - accuracy: 0.1112 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3019 - accuracy: 0.1111 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3020 - accuracy: 0.1104 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3019 - accuracy: 0.1107 - val_loss: 2.3018 - val_accuracy: 0.1010\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3018 - accuracy: 0.1010\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 16s 6ms/step - loss: 54512.9219 - accuracy: 0.1017 - val_loss: 2.3098 - val_accuracy: 0.0982\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3149 - accuracy: 0.1036 - val_loss: 2.3127 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3151 - accuracy: 0.1041 - val_loss: 2.3148 - val_accuracy: 0.0982\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3158 - accuracy: 0.1046 - val_loss: 2.3177 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3157 - accuracy: 0.1029 - val_loss: 2.3159 - val_accuracy: 0.0974\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3158 - accuracy: 0.1010 - val_loss: 2.3193 - val_accuracy: 0.1009\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3158 - accuracy: 0.1020 - val_loss: 2.3340 - val_accuracy: 0.1009\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3158 - accuracy: 0.1032 - val_loss: 2.3096 - val_accuracy: 0.1028\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3157 - accuracy: 0.1028 - val_loss: 2.3163 - val_accuracy: 0.0892\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3166 - accuracy: 0.1015 - val_loss: 2.3119 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3119 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3017 - accuracy: 0.1107 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3022 - accuracy: 0.1097 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3021 - accuracy: 0.1108 - val_loss: 2.3023 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3021 - accuracy: 0.1099 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3021 - accuracy: 0.1100 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3020 - accuracy: 0.1109 - val_loss: 2.3025 - val_accuracy: 0.1010\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3022 - accuracy: 0.1093 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3022 - accuracy: 0.1101 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3020 - accuracy: 0.1104 - val_loss: 2.3029 - val_accuracy: 0.0980\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3022 - accuracy: 0.1106 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3014 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 19s 7ms/step - loss: 1372226688.0000 - accuracy: 0.1026 - val_loss: 2.3087 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3161 - accuracy: 0.1030 - val_loss: 2.3167 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3169 - accuracy: 0.1015 - val_loss: 2.3131 - val_accuracy: 0.1010\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3166 - accuracy: 0.1014 - val_loss: 2.3052 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3162 - accuracy: 0.1018 - val_loss: 2.3217 - val_accuracy: 0.0982\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3158 - accuracy: 0.1034 - val_loss: 2.3188 - val_accuracy: 0.0892\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3162 - accuracy: 0.1016 - val_loss: 2.3178 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3166 - accuracy: 0.1024 - val_loss: 2.3137 - val_accuracy: 0.1032\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3164 - accuracy: 0.1036 - val_loss: 2.3125 - val_accuracy: 0.1009\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3167 - accuracy: 0.1001 - val_loss: 2.3107 - val_accuracy: 0.1028\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3107 - accuracy: 0.1028\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 373.5677 - accuracy: 0.1027 - val_loss: 2.3034 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3112 - accuracy: 0.1014 - val_loss: 2.3036 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3117 - accuracy: 0.1031 - val_loss: 2.3155 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3116 - accuracy: 0.1014 - val_loss: 2.3199 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3112 - accuracy: 0.1024 - val_loss: 2.3084 - val_accuracy: 0.0974\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3118 - accuracy: 0.1041 - val_loss: 2.3072 - val_accuracy: 0.0982\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3109 - accuracy: 0.1036 - val_loss: 2.3076 - val_accuracy: 0.0974\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3112 - accuracy: 0.1023 - val_loss: 2.3168 - val_accuracy: 0.1032\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3116 - accuracy: 0.1051 - val_loss: 2.3069 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3108 - accuracy: 0.1047 - val_loss: 2.3215 - val_accuracy: 0.0974\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3215 - accuracy: 0.0974\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3013 - accuracy: 0.1148 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3019 - accuracy: 0.1120 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3018 - accuracy: 0.1119 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3018 - accuracy: 0.1120 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3017 - accuracy: 0.1120 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3017 - accuracy: 0.1121 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3017 - accuracy: 0.1123 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3016 - accuracy: 0.1122 - val_loss: 2.3016 - val_accuracy: 0.1028\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3016 - accuracy: 0.1111 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 2.3016 - accuracy: 0.1122 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3013 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 10s 8ms/step - loss: 11735149568.0000 - accuracy: 0.1011 - val_loss: 2.3129 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3126 - accuracy: 0.1010 - val_loss: 2.3137 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 9s 8ms/step - loss: 2.3118 - accuracy: 0.1047 - val_loss: 2.3104 - val_accuracy: 0.0982\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3117 - accuracy: 0.1058 - val_loss: 2.3158 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 10s 8ms/step - loss: 2.3123 - accuracy: 0.1031 - val_loss: 2.3073 - val_accuracy: 0.0982\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3120 - accuracy: 0.1036 - val_loss: 2.3091 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3121 - accuracy: 0.1032 - val_loss: 2.3145 - val_accuracy: 0.0892\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3124 - accuracy: 0.1020 - val_loss: 2.3099 - val_accuracy: 0.0958\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3123 - accuracy: 0.1036 - val_loss: 2.3054 - val_accuracy: 0.1010\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 10s 8ms/step - loss: 2.3126 - accuracy: 0.1014 - val_loss: 2.3058 - val_accuracy: 0.0974\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3058 - accuracy: 0.0974\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 7s 10ms/step - loss: 912.5903 - accuracy: 0.1069 - val_loss: 2.3038 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3069 - accuracy: 0.1045 - val_loss: 2.3092 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3078 - accuracy: 0.1032 - val_loss: 2.3064 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3077 - accuracy: 0.1041 - val_loss: 2.3085 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3079 - accuracy: 0.1050 - val_loss: 2.3100 - val_accuracy: 0.0892\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3076 - accuracy: 0.1059 - val_loss: 2.3085 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3078 - accuracy: 0.1053 - val_loss: 2.3030 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3079 - accuracy: 0.1028 - val_loss: 2.3092 - val_accuracy: 0.0980\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3092 - accuracy: 0.1025 - val_loss: 2.3073 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3081 - accuracy: 0.1038 - val_loss: 2.3060 - val_accuracy: 0.1010\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3060 - accuracy: 0.1010\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 1.0422 - accuracy: 0.6375 - val_loss: 0.1335 - val_accuracy: 0.9587\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.1168 - accuracy: 0.9640 - val_loss: 0.0859 - val_accuracy: 0.9739\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0718 - accuracy: 0.9781 - val_loss: 0.0551 - val_accuracy: 0.9820\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0540 - accuracy: 0.9827 - val_loss: 0.0535 - val_accuracy: 0.9842\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0432 - accuracy: 0.9863 - val_loss: 0.0469 - val_accuracy: 0.9844\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 0.0450 - val_accuracy: 0.9866\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0309 - accuracy: 0.9899 - val_loss: 0.0423 - val_accuracy: 0.9865\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0450 - val_accuracy: 0.9855\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.0372 - val_accuracy: 0.9890\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.0565 - val_accuracy: 0.9829\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0565 - accuracy: 0.9829\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 7s 10ms/step - loss: 57905368.0000 - accuracy: 0.1013 - val_loss: 2.3109 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3088 - accuracy: 0.1057 - val_loss: 2.3097 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3093 - accuracy: 0.1030 - val_loss: 2.3078 - val_accuracy: 0.0982\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3094 - accuracy: 0.1016 - val_loss: 2.3072 - val_accuracy: 0.0980\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3094 - accuracy: 0.1054 - val_loss: 2.3051 - val_accuracy: 0.1009\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3092 - accuracy: 0.1065 - val_loss: 2.3120 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3091 - accuracy: 0.1043 - val_loss: 2.3063 - val_accuracy: 0.0982\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 6s 11ms/step - loss: 2.3093 - accuracy: 0.1056 - val_loss: 2.3064 - val_accuracy: 0.0982\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3086 - accuracy: 0.1040 - val_loss: 2.3098 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 6s 10ms/step - loss: 2.3089 - accuracy: 0.1044 - val_loss: 2.3089 - val_accuracy: 0.0892\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3089 - accuracy: 0.0892\n"
          ]
        }
      ],
      "source": [
        "a = []\n",
        "d = []\n",
        "c = []\n",
        "e = []\n",
        "\n",
        "# define the model for the convolutional neural net for INCREASING filters\n",
        "for lr in learning_rate:\n",
        "    for bs in batch_size:\n",
        "        for oc in optimizer_choice:\n",
        "            # Training the layers for the CNN using Keras\n",
        "            # The three convolutional layers\n",
        "            model = tf.keras.models.Sequential()\n",
        "            model.add(tf.keras.layers.Conv2D(2, (3, 3), kernel_initializer='he_uniform', activation='relu', input_shape=(28, 28, 1)))\n",
        "            model.add(tf.keras.layers.Conv2D(4, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(8, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(24, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(48, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(86, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(92, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "            # Flatten the convulational layers and add the dense layers with 84 neurons and the relu activation\n",
        "            # and the output layer with the softmax output layer with 10 nominal output.\n",
        "            model.add(tf.keras.layers.Flatten())\n",
        "            model.add(tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform')) # layer 9\n",
        "            model.add(tf.keras.layers.Dense(10, activation='softmax')) # layer 10\n",
        "\n",
        "            # Compile\n",
        "            opt = oc(learning_rate = lr)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "            # Fit the training data to the model\n",
        "            NNmodel = model.fit(x_train, y_train, batch_size=bs, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "            # lists\n",
        "            a.append(lr)\n",
        "            d.append(bs)\n",
        "            c.append(oc)\n",
        "            e.append(model.evaluate(x_test, y_test)[1])\n",
        "\n",
        "models1 = pd.DataFrame({'Learning Rate': a,\n",
        "                        'Batch Size': d,\n",
        "                        'Optimizer': c,\n",
        "                        'Accuracy': e})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "hSElQUOAu9FW",
        "outputId": "21a18b5d-41d7-4ac8-cbcf-19dc6007c315"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Learning Rate  Batch Size  \\\n",
              "6           0.001         100   \n",
              "3           0.001          50   \n",
              "8           0.001         100   \n",
              "13          0.010          50   \n",
              "10          0.010          25   \n",
              "16          0.010         100   \n",
              "0           0.001          25   \n",
              "5           0.001          50   \n",
              "25          0.100         100   \n",
              "1           0.001          25   \n",
              "4           0.001          50   \n",
              "15          0.010         100   \n",
              "12          0.010          50   \n",
              "2           0.001          25   \n",
              "9           0.010          25   \n",
              "7           0.001         100   \n",
              "22          0.100          50   \n",
              "14          0.010          50   \n",
              "18          0.100          25   \n",
              "19          0.100          25   \n",
              "20          0.100          25   \n",
              "24          0.100         100   \n",
              "17          0.010         100   \n",
              "11          0.010          25   \n",
              "23          0.100          50   \n",
              "21          0.100          50   \n",
              "26          0.100         100   \n",
              "\n",
              "                                            Optimizer  Accuracy  \n",
              "6   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9914  \n",
              "3   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9904  \n",
              "8   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9899  \n",
              "13  <class 'keras.optimizers.optimizer_v2.gradient...    0.9879  \n",
              "10  <class 'keras.optimizers.optimizer_v2.gradient...    0.9873  \n",
              "16  <class 'keras.optimizers.optimizer_v2.gradient...    0.9853  \n",
              "0   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9851  \n",
              "5   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9850  \n",
              "25  <class 'keras.optimizers.optimizer_v2.gradient...    0.9829  \n",
              "1   <class 'keras.optimizers.optimizer_v2.gradient...    0.9816  \n",
              "4   <class 'keras.optimizers.optimizer_v2.gradient...    0.9728  \n",
              "15  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9687  \n",
              "12  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9613  \n",
              "2   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9540  \n",
              "9   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9522  \n",
              "7   <class 'keras.optimizers.optimizer_v2.gradient...    0.9372  \n",
              "22  <class 'keras.optimizers.optimizer_v2.gradient...    0.1135  \n",
              "14  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "18  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "19  <class 'keras.optimizers.optimizer_v2.gradient...    0.1135  \n",
              "20  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1028  \n",
              "24  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1010  \n",
              "17  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1010  \n",
              "11  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1010  \n",
              "23  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.0974  \n",
              "21  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.0974  \n",
              "26  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.0892  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ffbf63b-b941-4b21-83d2-51e7f5997fff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.0974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.0974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.0892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ffbf63b-b941-4b21-83d2-51e7f5997fff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ffbf63b-b941-4b21-83d2-51e7f5997fff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ffbf63b-b941-4b21-83d2-51e7f5997fff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "models1.sort_values('Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "abWNI3f-GzI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e53bb44-7915-4108-bef9-a74831f3d158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 0.4789 - accuracy: 0.8531 - val_loss: 0.1939 - val_accuracy: 0.9431\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.1821 - accuracy: 0.9465 - val_loss: 0.1376 - val_accuracy: 0.9592\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.1338 - accuracy: 0.9600 - val_loss: 0.1283 - val_accuracy: 0.9600\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.1098 - accuracy: 0.9671 - val_loss: 0.0998 - val_accuracy: 0.9717\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0936 - accuracy: 0.9719 - val_loss: 0.1046 - val_accuracy: 0.9699\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.0817 - accuracy: 0.9758 - val_loss: 0.0797 - val_accuracy: 0.9752\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0738 - accuracy: 0.9776 - val_loss: 0.0851 - val_accuracy: 0.9746\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.0665 - accuracy: 0.9801 - val_loss: 0.0713 - val_accuracy: 0.9778\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.0626 - accuracy: 0.9807 - val_loss: 0.0691 - val_accuracy: 0.9795\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.0566 - accuracy: 0.9830 - val_loss: 0.0667 - val_accuracy: 0.9808\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0667 - accuracy: 0.9808\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 1.4201 - accuracy: 0.5085 - val_loss: 0.5240 - val_accuracy: 0.8326\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.3737 - accuracy: 0.8867 - val_loss: 0.2408 - val_accuracy: 0.9258\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.2380 - accuracy: 0.9282 - val_loss: 0.1930 - val_accuracy: 0.9402\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1870 - accuracy: 0.9426 - val_loss: 0.1558 - val_accuracy: 0.9528\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1567 - accuracy: 0.9528 - val_loss: 0.1686 - val_accuracy: 0.9458\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.1398 - accuracy: 0.9580 - val_loss: 0.1162 - val_accuracy: 0.9631\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1241 - accuracy: 0.9623 - val_loss: 0.1092 - val_accuracy: 0.9669\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1128 - accuracy: 0.9660 - val_loss: 0.1151 - val_accuracy: 0.9655\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.1038 - accuracy: 0.9689 - val_loss: 0.0913 - val_accuracy: 0.9706\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0983 - accuracy: 0.9701 - val_loss: 0.0864 - val_accuracy: 0.9718\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0864 - accuracy: 0.9718\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 21s 8ms/step - loss: 0.5090 - accuracy: 0.8379 - val_loss: 0.1704 - val_accuracy: 0.9483\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1794 - accuracy: 0.9484 - val_loss: 0.1156 - val_accuracy: 0.9632\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1389 - accuracy: 0.9596 - val_loss: 0.1342 - val_accuracy: 0.9629\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1178 - accuracy: 0.9665 - val_loss: 0.1036 - val_accuracy: 0.9709\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1108 - accuracy: 0.9683 - val_loss: 0.0787 - val_accuracy: 0.9765\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1050 - accuracy: 0.9705 - val_loss: 0.0908 - val_accuracy: 0.9730\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1028 - accuracy: 0.9711 - val_loss: 0.0891 - val_accuracy: 0.9732\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.0989 - accuracy: 0.9722 - val_loss: 0.0774 - val_accuracy: 0.9761\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1020 - accuracy: 0.9726 - val_loss: 0.1203 - val_accuracy: 0.9667\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.1078 - accuracy: 0.9740 - val_loss: 0.0775 - val_accuracy: 0.9770\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0775 - accuracy: 0.9770\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 16s 12ms/step - loss: 0.4683 - accuracy: 0.8536 - val_loss: 0.1461 - val_accuracy: 0.9553\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.1422 - accuracy: 0.9578 - val_loss: 0.1047 - val_accuracy: 0.9695\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0991 - accuracy: 0.9695 - val_loss: 0.1048 - val_accuracy: 0.9678\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0820 - accuracy: 0.9746 - val_loss: 0.0741 - val_accuracy: 0.9755\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0686 - accuracy: 0.9786 - val_loss: 0.0634 - val_accuracy: 0.9806\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0650 - accuracy: 0.9799 - val_loss: 0.0624 - val_accuracy: 0.9807\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0546 - accuracy: 0.9827 - val_loss: 0.0664 - val_accuracy: 0.9797\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0491 - accuracy: 0.9846 - val_loss: 0.0550 - val_accuracy: 0.9820\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0463 - accuracy: 0.9852 - val_loss: 0.0643 - val_accuracy: 0.9816\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0429 - accuracy: 0.9872 - val_loss: 0.0491 - val_accuracy: 0.9849\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0491 - accuracy: 0.9849\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 16s 13ms/step - loss: 1.8657 - accuracy: 0.3443 - val_loss: 1.2063 - val_accuracy: 0.5951\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.7318 - accuracy: 0.7772 - val_loss: 0.4226 - val_accuracy: 0.8750\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.3867 - accuracy: 0.8874 - val_loss: 0.2910 - val_accuracy: 0.9174\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.2838 - accuracy: 0.9165 - val_loss: 0.2253 - val_accuracy: 0.9340\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.2333 - accuracy: 0.9313 - val_loss: 0.1933 - val_accuracy: 0.9439\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.2018 - accuracy: 0.9399 - val_loss: 0.1716 - val_accuracy: 0.9492\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.1797 - accuracy: 0.9471 - val_loss: 0.1711 - val_accuracy: 0.9469\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.1637 - accuracy: 0.9510 - val_loss: 0.1603 - val_accuracy: 0.9522\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.1500 - accuracy: 0.9546 - val_loss: 0.1357 - val_accuracy: 0.9558\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.1382 - accuracy: 0.9576 - val_loss: 0.1360 - val_accuracy: 0.9596\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.1360 - accuracy: 0.9596\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 16s 13ms/step - loss: 0.5075 - accuracy: 0.8331 - val_loss: 0.1734 - val_accuracy: 0.9467\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.1646 - accuracy: 0.9506 - val_loss: 0.1641 - val_accuracy: 0.9494\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 0.1220 - accuracy: 0.9638 - val_loss: 0.0949 - val_accuracy: 0.9713\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 0.1006 - accuracy: 0.9697 - val_loss: 0.0985 - val_accuracy: 0.9740\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0904 - accuracy: 0.9738 - val_loss: 0.0796 - val_accuracy: 0.9775\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0812 - accuracy: 0.9760 - val_loss: 0.0885 - val_accuracy: 0.9765\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 0.0758 - accuracy: 0.9776 - val_loss: 0.0719 - val_accuracy: 0.9776\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0697 - accuracy: 0.9790 - val_loss: 0.0667 - val_accuracy: 0.9796\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0641 - accuracy: 0.9807 - val_loss: 0.0739 - val_accuracy: 0.9782\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0626 - accuracy: 0.9814 - val_loss: 0.0668 - val_accuracy: 0.9801\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.0668 - accuracy: 0.9801\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 15s 23ms/step - loss: 0.4789 - accuracy: 0.8451 - val_loss: 0.1973 - val_accuracy: 0.9409\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.1614 - accuracy: 0.9509 - val_loss: 0.1473 - val_accuracy: 0.9551\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.1189 - accuracy: 0.9644 - val_loss: 0.1131 - val_accuracy: 0.9643\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0951 - accuracy: 0.9708 - val_loss: 0.1002 - val_accuracy: 0.9702\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.0851 - accuracy: 0.9746 - val_loss: 0.0768 - val_accuracy: 0.9781\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.0759 - accuracy: 0.9768 - val_loss: 0.0679 - val_accuracy: 0.9806\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.0688 - accuracy: 0.9785 - val_loss: 0.0770 - val_accuracy: 0.9763\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0647 - accuracy: 0.9803 - val_loss: 0.0700 - val_accuracy: 0.9786\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0569 - accuracy: 0.9821 - val_loss: 0.0644 - val_accuracy: 0.9809\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0534 - accuracy: 0.9837 - val_loss: 0.0648 - val_accuracy: 0.9806\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0648 - accuracy: 0.9806\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 14s 23ms/step - loss: 1.9877 - accuracy: 0.2728 - val_loss: 1.3050 - val_accuracy: 0.5771\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.9610 - accuracy: 0.7077 - val_loss: 0.6298 - val_accuracy: 0.8146\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.5976 - accuracy: 0.8227 - val_loss: 0.4548 - val_accuracy: 0.8624\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.4598 - accuracy: 0.8653 - val_loss: 0.3589 - val_accuracy: 0.8965\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.3812 - accuracy: 0.8891 - val_loss: 0.3152 - val_accuracy: 0.9061\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.3235 - accuracy: 0.9050 - val_loss: 0.2666 - val_accuracy: 0.9204\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.2847 - accuracy: 0.9171 - val_loss: 0.2270 - val_accuracy: 0.9326\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.2525 - accuracy: 0.9257 - val_loss: 0.2118 - val_accuracy: 0.9347\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.2309 - accuracy: 0.9319 - val_loss: 0.1935 - val_accuracy: 0.9417\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.2127 - accuracy: 0.9370 - val_loss: 0.1927 - val_accuracy: 0.9417\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1927 - accuracy: 0.9417\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 15s 23ms/step - loss: 0.5983 - accuracy: 0.8103 - val_loss: 0.1667 - val_accuracy: 0.9491\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.1752 - accuracy: 0.9486 - val_loss: 0.1014 - val_accuracy: 0.9694\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.1178 - accuracy: 0.9660 - val_loss: 0.0913 - val_accuracy: 0.9725\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0955 - accuracy: 0.9725 - val_loss: 0.0818 - val_accuracy: 0.9769\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 14s 23ms/step - loss: 0.0821 - accuracy: 0.9756 - val_loss: 0.0755 - val_accuracy: 0.9777\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0746 - accuracy: 0.9788 - val_loss: 0.0599 - val_accuracy: 0.9819\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0647 - accuracy: 0.9812 - val_loss: 0.0638 - val_accuracy: 0.9816\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0614 - accuracy: 0.9823 - val_loss: 0.0695 - val_accuracy: 0.9795\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0541 - accuracy: 0.9831 - val_loss: 0.0651 - val_accuracy: 0.9800\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 0.0595 - val_accuracy: 0.9838\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0595 - accuracy: 0.9838\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3026 - accuracy: 0.1087 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3028 - accuracy: 0.1072 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3027 - accuracy: 0.1106 - val_loss: 2.3023 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3027 - accuracy: 0.1085 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3027 - accuracy: 0.1095 - val_loss: 2.3018 - val_accuracy: 0.1028\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3026 - accuracy: 0.1086 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3025 - accuracy: 0.1077 - val_loss: 2.3034 - val_accuracy: 0.1009\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3028 - accuracy: 0.1100 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3027 - accuracy: 0.1076 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3024 - accuracy: 0.1107 - val_loss: 2.3027 - val_accuracy: 0.1028\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3027 - accuracy: 0.1028\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 0.5850 - accuracy: 0.8096 - val_loss: 0.1804 - val_accuracy: 0.9445\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.1708 - accuracy: 0.9482 - val_loss: 0.1291 - val_accuracy: 0.9590\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.1082 - accuracy: 0.9669 - val_loss: 0.0865 - val_accuracy: 0.9722\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 0.0836 - accuracy: 0.9743 - val_loss: 0.0738 - val_accuracy: 0.9762\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0693 - accuracy: 0.9783 - val_loss: 0.0617 - val_accuracy: 0.9818\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 0.0605 - accuracy: 0.9808 - val_loss: 0.0671 - val_accuracy: 0.9777\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0521 - accuracy: 0.9840 - val_loss: 0.0558 - val_accuracy: 0.9819\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0481 - accuracy: 0.9850 - val_loss: 0.0782 - val_accuracy: 0.9751\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0434 - accuracy: 0.9865 - val_loss: 0.0425 - val_accuracy: 0.9860\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.0397 - accuracy: 0.9875 - val_loss: 0.0534 - val_accuracy: 0.9839\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.0534 - accuracy: 0.9839\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 21s 8ms/step - loss: 5.3275 - accuracy: 0.1078 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 21s 9ms/step - loss: 2.3026 - accuracy: 0.1085 - val_loss: 2.3024 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3026 - accuracy: 0.1100 - val_loss: 2.3030 - val_accuracy: 0.0974\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3026 - accuracy: 0.1079 - val_loss: 2.3027 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3026 - accuracy: 0.1102 - val_loss: 2.3027 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 20s 9ms/step - loss: 2.3028 - accuracy: 0.1088 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3028 - accuracy: 0.1090 - val_loss: 2.3025 - val_accuracy: 0.1009\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 20s 9ms/step - loss: 2.3028 - accuracy: 0.1093 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 21s 9ms/step - loss: 2.3028 - accuracy: 0.1095 - val_loss: 2.3029 - val_accuracy: 0.1009\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3027 - accuracy: 0.1093 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 2.3014 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3037 - accuracy: 0.1085 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3022 - accuracy: 0.1092 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3022 - accuracy: 0.1111 - val_loss: 2.3021 - val_accuracy: 0.1028\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3023 - accuracy: 0.1090 - val_loss: 2.3027 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3023 - accuracy: 0.1111 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3023 - accuracy: 0.1099 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3024 - accuracy: 0.1109 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3022 - accuracy: 0.1092 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3023 - accuracy: 0.1099 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3022 - accuracy: 0.1094 - val_loss: 2.3025 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3025 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.6686 - accuracy: 0.7833 - val_loss: 0.2481 - val_accuracy: 0.9235\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.2001 - accuracy: 0.9416 - val_loss: 0.2200 - val_accuracy: 0.9330\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.1393 - accuracy: 0.9585 - val_loss: 0.1092 - val_accuracy: 0.9688\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.1117 - accuracy: 0.9673 - val_loss: 0.1065 - val_accuracy: 0.9682\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0962 - accuracy: 0.9716 - val_loss: 0.0815 - val_accuracy: 0.9759\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0837 - accuracy: 0.9750 - val_loss: 0.0728 - val_accuracy: 0.9767\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0752 - accuracy: 0.9774 - val_loss: 0.0881 - val_accuracy: 0.9729\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0696 - accuracy: 0.9787 - val_loss: 0.0678 - val_accuracy: 0.9782\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 0.0626 - accuracy: 0.9815 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 0.0571 - accuracy: 0.9826 - val_loss: 0.0813 - val_accuracy: 0.9755\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0813 - accuracy: 0.9755\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 16s 12ms/step - loss: 2.6181 - accuracy: 0.1097 - val_loss: 2.3022 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3022 - accuracy: 0.1094 - val_loss: 2.3027 - val_accuracy: 0.1028\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3020 - accuracy: 0.1105 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3024 - accuracy: 0.1105 - val_loss: 2.3019 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3023 - accuracy: 0.1098 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3021 - accuracy: 0.1112 - val_loss: 2.3022 - val_accuracy: 0.1009\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 2.3024 - accuracy: 0.1093 - val_loss: 2.3028 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3023 - accuracy: 0.1103 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3022 - accuracy: 0.1106 - val_loss: 2.3025 - val_accuracy: 0.1028\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3021 - accuracy: 0.1097 - val_loss: 2.3029 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3029 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 14s 22ms/step - loss: 2.3022 - accuracy: 0.1104 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3019 - accuracy: 0.1116 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3021 - accuracy: 0.1111 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3021 - accuracy: 0.1107 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3021 - accuracy: 0.1118 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3021 - accuracy: 0.1108 - val_loss: 2.3015 - val_accuracy: 0.1028\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3021 - accuracy: 0.1102 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3019 - accuracy: 0.1112 - val_loss: 2.3017 - val_accuracy: 0.1032\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3019 - accuracy: 0.1109 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3019 - accuracy: 0.1110 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 2.3013 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 14s 23ms/step - loss: 0.8945 - accuracy: 0.7039 - val_loss: 0.3443 - val_accuracy: 0.8947\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.2548 - accuracy: 0.9253 - val_loss: 0.1678 - val_accuracy: 0.9521\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.1711 - accuracy: 0.9492 - val_loss: 0.1252 - val_accuracy: 0.9621\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.1368 - accuracy: 0.9602 - val_loss: 0.1157 - val_accuracy: 0.9657\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.1114 - accuracy: 0.9667 - val_loss: 0.0964 - val_accuracy: 0.9704\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0980 - accuracy: 0.9717 - val_loss: 0.0834 - val_accuracy: 0.9770\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.0888 - accuracy: 0.9737 - val_loss: 0.0862 - val_accuracy: 0.9750\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0790 - accuracy: 0.9762 - val_loss: 0.0827 - val_accuracy: 0.9758\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.0717 - accuracy: 0.9793 - val_loss: 0.0790 - val_accuracy: 0.9766\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.0657 - accuracy: 0.9807 - val_loss: 0.0651 - val_accuracy: 0.9816\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.0651 - accuracy: 0.9816\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 14s 21ms/step - loss: 5.3873 - accuracy: 0.1096 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3019 - accuracy: 0.1108 - val_loss: 2.3023 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3020 - accuracy: 0.1116 - val_loss: 2.3023 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3019 - accuracy: 0.1104 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3021 - accuracy: 0.1111 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3019 - accuracy: 0.1115 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3020 - accuracy: 0.1113 - val_loss: 2.3015 - val_accuracy: 0.1028\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3020 - accuracy: 0.1105 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3019 - accuracy: 0.1110 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3019 - accuracy: 0.1111 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3010 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3161 - accuracy: 0.1036 - val_loss: 2.3098 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3162 - accuracy: 0.1028 - val_loss: 2.3089 - val_accuracy: 0.1032\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3152 - accuracy: 0.1031 - val_loss: 2.3090 - val_accuracy: 0.1032\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3153 - accuracy: 0.1031 - val_loss: 2.3119 - val_accuracy: 0.1032\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3167 - accuracy: 0.1023 - val_loss: 2.3085 - val_accuracy: 0.0974\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3157 - accuracy: 0.1041 - val_loss: 2.3212 - val_accuracy: 0.0892\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3160 - accuracy: 0.1027 - val_loss: 2.3159 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3156 - accuracy: 0.1007 - val_loss: 2.3148 - val_accuracy: 0.1028\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3159 - accuracy: 0.1013 - val_loss: 2.3192 - val_accuracy: 0.1028\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3159 - accuracy: 0.1021 - val_loss: 2.3137 - val_accuracy: 0.0982\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 2.3137 - accuracy: 0.0982\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3029 - accuracy: 0.1099 - val_loss: 2.3024 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3022 - accuracy: 0.1094 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3022 - accuracy: 0.1107 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3022 - accuracy: 0.1105 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3022 - accuracy: 0.1103 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3021 - accuracy: 0.1115 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3022 - accuracy: 0.1112 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 19s 8ms/step - loss: 2.3020 - accuracy: 0.1104 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3022 - accuracy: 0.1109 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3021 - accuracy: 0.1109 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3021 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 22s 9ms/step - loss: 53590228.0000 - accuracy: 0.1023 - val_loss: 2.3132 - val_accuracy: 0.1009\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3166 - accuracy: 0.1016 - val_loss: 2.3066 - val_accuracy: 0.0892\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3166 - accuracy: 0.1035 - val_loss: 2.3196 - val_accuracy: 0.1028\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3152 - accuracy: 0.1064 - val_loss: 2.3347 - val_accuracy: 0.1009\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3166 - accuracy: 0.1032 - val_loss: 2.3186 - val_accuracy: 0.1010\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3166 - accuracy: 0.1031 - val_loss: 2.3191 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 21s 9ms/step - loss: 2.3164 - accuracy: 0.1028 - val_loss: 2.3385 - val_accuracy: 0.0980\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 21s 9ms/step - loss: 2.3165 - accuracy: 0.1015 - val_loss: 2.3327 - val_accuracy: 0.0974\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 20s 9ms/step - loss: 2.3168 - accuracy: 0.1025 - val_loss: 2.3100 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 20s 8ms/step - loss: 2.3166 - accuracy: 0.1002 - val_loss: 2.3167 - val_accuracy: 0.1028\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 2.3167 - accuracy: 0.1028\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3120 - accuracy: 0.1038 - val_loss: 2.3066 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3110 - accuracy: 0.1028 - val_loss: 2.3062 - val_accuracy: 0.1032\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3114 - accuracy: 0.1038 - val_loss: 2.3144 - val_accuracy: 0.0974\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3110 - accuracy: 0.1050 - val_loss: 2.3046 - val_accuracy: 0.1032\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3107 - accuracy: 0.1055 - val_loss: 2.3084 - val_accuracy: 0.0980\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3111 - accuracy: 0.1005 - val_loss: 2.3054 - val_accuracy: 0.1032\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3113 - accuracy: 0.1050 - val_loss: 2.3067 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3112 - accuracy: 0.1046 - val_loss: 2.3102 - val_accuracy: 0.0982\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3100 - accuracy: 0.1030 - val_loss: 2.3086 - val_accuracy: 0.0958\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3106 - accuracy: 0.1026 - val_loss: 2.3090 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3090 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3026 - accuracy: 0.1136 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 14s 11ms/step - loss: 2.3016 - accuracy: 0.1120 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3017 - accuracy: 0.1121 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3016 - accuracy: 0.1120 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3016 - accuracy: 0.1120 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 11ms/step - loss: 2.3018 - accuracy: 0.1122 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 14s 11ms/step - loss: 2.3017 - accuracy: 0.1122 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3018 - accuracy: 0.1120 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 14s 11ms/step - loss: 2.3017 - accuracy: 0.1115 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 14s 11ms/step - loss: 2.3016 - accuracy: 0.1129 - val_loss: 2.3021 - val_accuracy: 0.1028\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3021 - accuracy: 0.1028\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 16s 12ms/step - loss: 24134504.0000 - accuracy: 0.1007 - val_loss: 2.3150 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 2.3123 - accuracy: 0.1034 - val_loss: 2.3071 - val_accuracy: 0.0980\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 2.3121 - accuracy: 0.1019 - val_loss: 2.3143 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 2.3121 - accuracy: 0.1037 - val_loss: 2.3119 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3118 - accuracy: 0.1019 - val_loss: 2.3128 - val_accuracy: 0.1028\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 14s 12ms/step - loss: 2.3121 - accuracy: 0.1038 - val_loss: 2.3179 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 15s 13ms/step - loss: 2.3123 - accuracy: 0.1035 - val_loss: 2.3186 - val_accuracy: 0.0958\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3123 - accuracy: 0.1004 - val_loss: 2.3092 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3118 - accuracy: 0.1050 - val_loss: 2.3070 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 15s 12ms/step - loss: 2.3119 - accuracy: 0.1035 - val_loss: 2.3071 - val_accuracy: 0.1032\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3071 - accuracy: 0.1032\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 208054.6875 - accuracy: 0.1053 - val_loss: 2.3055 - val_accuracy: 0.1010\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3082 - accuracy: 0.1022 - val_loss: 2.3118 - val_accuracy: 0.1009\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3085 - accuracy: 0.1035 - val_loss: 2.3070 - val_accuracy: 0.1010\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3075 - accuracy: 0.1039 - val_loss: 2.3057 - val_accuracy: 0.0958\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3081 - accuracy: 0.1046 - val_loss: 2.3040 - val_accuracy: 0.1009\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3077 - accuracy: 0.1031 - val_loss: 2.3066 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3080 - accuracy: 0.1054 - val_loss: 2.3090 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3083 - accuracy: 0.1030 - val_loss: 2.3085 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3083 - accuracy: 0.1042 - val_loss: 2.3108 - val_accuracy: 0.1028\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3087 - accuracy: 0.1047 - val_loss: 2.3188 - val_accuracy: 0.0980\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.3188 - accuracy: 0.0980\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3099 - accuracy: 0.1154 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 12s 21ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 2.3011 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 15s 23ms/step - loss: 1625843.8750 - accuracy: 0.1047 - val_loss: 2.3069 - val_accuracy: 0.1010\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3092 - accuracy: 0.1034 - val_loss: 2.3066 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3087 - accuracy: 0.1046 - val_loss: 2.3115 - val_accuracy: 0.0958\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3089 - accuracy: 0.1047 - val_loss: 2.3090 - val_accuracy: 0.0958\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3097 - accuracy: 0.1034 - val_loss: 2.3130 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3087 - accuracy: 0.1051 - val_loss: 2.3076 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 2.3090 - accuracy: 0.1041 - val_loss: 2.3169 - val_accuracy: 0.0958\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3092 - accuracy: 0.1035 - val_loss: 2.3157 - val_accuracy: 0.1028\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3094 - accuracy: 0.1032 - val_loss: 2.3070 - val_accuracy: 0.1028\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 2.3087 - accuracy: 0.1054 - val_loss: 2.3079 - val_accuracy: 0.1009\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 2.3079 - accuracy: 0.1009\n"
          ]
        }
      ],
      "source": [
        "a = []\n",
        "d = []\n",
        "c = []\n",
        "e = []\n",
        "\n",
        "# define the model for the convolutional neural net for REDUCING filters\n",
        "for lr in learning_rate:\n",
        "    for bs in batch_size:\n",
        "        for oc in optimizer_choice:\n",
        "            # Training the layers for the CNN using Keras\n",
        "            # # The three convolutional layers\n",
        "            model = tf.keras.models.Sequential()\n",
        "            model.add(tf.keras.layers.Conv2D(92, (3, 3), kernel_initializer='he_uniform', activation='relu', input_shape=(28, 28, 1)))\n",
        "            model.add(tf.keras.layers.Conv2D(86, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(48, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(24, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(8, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(4, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(2, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "            # Flatten the convulational layers and add the dense layers with 84 neurons and the relu activation\n",
        "            # and the output layer with the softmax output layer with 10 nominal output.\n",
        "            model.add(tf.keras.layers.Flatten())\n",
        "            model.add(tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform')) # layer 9\n",
        "            model.add(tf.keras.layers.Dense(10, activation='softmax')) # layer 10\n",
        "\n",
        "            # Compile\n",
        "            opt = oc(learning_rate = lr)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "            # Fit the training data to the model\n",
        "            NNmodel = model.fit(x_train, y_train, batch_size=bs, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "            # lists\n",
        "            a.append(lr)\n",
        "            d.append(bs)\n",
        "            c.append(oc)\n",
        "            e.append(model.evaluate(x_test, y_test)[1])\n",
        "\n",
        "models2 = pd.DataFrame({'Learning Rate': a,\n",
        "                        'Batch Size': d,\n",
        "                        'Optimizer': c,\n",
        "                        'Accuracy': e})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "3tKL69be3lH0",
        "outputId": "f1140f0c-38a5-47d1-ad47-1b115b15e067"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Learning Rate  Batch Size  \\\n",
              "3           0.001          50   \n",
              "10          0.010          25   \n",
              "8           0.001         100   \n",
              "16          0.010         100   \n",
              "0           0.001          25   \n",
              "6           0.001         100   \n",
              "5           0.001          50   \n",
              "2           0.001          25   \n",
              "13          0.010          50   \n",
              "1           0.001          25   \n",
              "4           0.001          50   \n",
              "7           0.001         100   \n",
              "25          0.100         100   \n",
              "11          0.010          25   \n",
              "12          0.010          50   \n",
              "14          0.010          50   \n",
              "15          0.010         100   \n",
              "17          0.010         100   \n",
              "19          0.100          25   \n",
              "21          0.100          50   \n",
              "23          0.100          50   \n",
              "9           0.010          25   \n",
              "20          0.100          25   \n",
              "22          0.100          50   \n",
              "26          0.100         100   \n",
              "18          0.100          25   \n",
              "24          0.100         100   \n",
              "\n",
              "                                            Optimizer  Accuracy  \n",
              "3   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9849  \n",
              "10  <class 'keras.optimizers.optimizer_v2.gradient...    0.9839  \n",
              "8   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9838  \n",
              "16  <class 'keras.optimizers.optimizer_v2.gradient...    0.9816  \n",
              "0   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9808  \n",
              "6   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9806  \n",
              "5   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9801  \n",
              "2   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9770  \n",
              "13  <class 'keras.optimizers.optimizer_v2.gradient...    0.9755  \n",
              "1   <class 'keras.optimizers.optimizer_v2.gradient...    0.9718  \n",
              "4   <class 'keras.optimizers.optimizer_v2.gradient...    0.9596  \n",
              "7   <class 'keras.optimizers.optimizer_v2.gradient...    0.9417  \n",
              "25  <class 'keras.optimizers.optimizer_v2.gradient...    0.1135  \n",
              "11  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "12  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "14  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "15  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "17  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "19  <class 'keras.optimizers.optimizer_v2.gradient...    0.1135  \n",
              "21  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "23  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1032  \n",
              "9   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1028  \n",
              "20  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1028  \n",
              "22  <class 'keras.optimizers.optimizer_v2.gradient...    0.1028  \n",
              "26  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1009  \n",
              "18  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.0982  \n",
              "24  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.0980  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-92a78c0c-2388-4f77-9cf9-aeec6031164e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.0982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.0980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92a78c0c-2388-4f77-9cf9-aeec6031164e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-92a78c0c-2388-4f77-9cf9-aeec6031164e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-92a78c0c-2388-4f77-9cf9-aeec6031164e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "models2.sort_values('Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W34_jtxuJnOs",
        "outputId": "c038fc9e-f77e-429f-b44f-10b07f1cec15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.3111 - accuracy: 0.9031 - val_loss: 0.1445 - val_accuracy: 0.9576\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.1304 - accuracy: 0.9602 - val_loss: 0.1540 - val_accuracy: 0.9527\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0963 - accuracy: 0.9701 - val_loss: 0.0765 - val_accuracy: 0.9759\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0806 - accuracy: 0.9750 - val_loss: 0.1096 - val_accuracy: 0.9686\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0685 - accuracy: 0.9786 - val_loss: 0.0657 - val_accuracy: 0.9797\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0621 - accuracy: 0.9807 - val_loss: 0.0622 - val_accuracy: 0.9804\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0556 - accuracy: 0.9828 - val_loss: 0.0556 - val_accuracy: 0.9828\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0535 - accuracy: 0.9831 - val_loss: 0.0505 - val_accuracy: 0.9830\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 0.0482 - accuracy: 0.9854 - val_loss: 0.0581 - val_accuracy: 0.9800\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.0446 - accuracy: 0.9864 - val_loss: 0.0494 - val_accuracy: 0.9834\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0494 - accuracy: 0.9834\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 14s 5ms/step - loss: 1.7727 - accuracy: 0.3660 - val_loss: 0.6181 - val_accuracy: 0.7999\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.4142 - accuracy: 0.8706 - val_loss: 0.2609 - val_accuracy: 0.9201\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.2558 - accuracy: 0.9226 - val_loss: 0.2127 - val_accuracy: 0.9332\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 0.2035 - accuracy: 0.9396 - val_loss: 0.1529 - val_accuracy: 0.9526\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1753 - accuracy: 0.9474 - val_loss: 0.1445 - val_accuracy: 0.9545\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 0.1578 - accuracy: 0.9532 - val_loss: 0.1246 - val_accuracy: 0.9604\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1428 - accuracy: 0.9574 - val_loss: 0.1549 - val_accuracy: 0.9508\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1327 - accuracy: 0.9612 - val_loss: 0.1081 - val_accuracy: 0.9651\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1224 - accuracy: 0.9634 - val_loss: 0.1017 - val_accuracy: 0.9675\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1147 - accuracy: 0.9653 - val_loss: 0.0976 - val_accuracy: 0.9689\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0976 - accuracy: 0.9689\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 0.3347 - accuracy: 0.8938 - val_loss: 0.1840 - val_accuracy: 0.9427\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.1306 - accuracy: 0.9610 - val_loss: 0.0940 - val_accuracy: 0.9676\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0981 - accuracy: 0.9711 - val_loss: 0.0693 - val_accuracy: 0.9771\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0860 - accuracy: 0.9743 - val_loss: 0.0844 - val_accuracy: 0.9727\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0777 - accuracy: 0.9768 - val_loss: 0.0549 - val_accuracy: 0.9817\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0737 - accuracy: 0.9788 - val_loss: 0.0792 - val_accuracy: 0.9768\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0701 - accuracy: 0.9793 - val_loss: 0.0606 - val_accuracy: 0.9816\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0674 - accuracy: 0.9807 - val_loss: 0.0561 - val_accuracy: 0.9818\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0673 - accuracy: 0.9803 - val_loss: 0.0625 - val_accuracy: 0.9835\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 0.0663 - accuracy: 0.9817 - val_loss: 0.0622 - val_accuracy: 0.9828\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9828\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3289 - accuracy: 0.8963 - val_loss: 0.1228 - val_accuracy: 0.9619\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.1131 - accuracy: 0.9656 - val_loss: 0.0785 - val_accuracy: 0.9764\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.0849 - accuracy: 0.9736 - val_loss: 0.0753 - val_accuracy: 0.9743\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0726 - accuracy: 0.9775 - val_loss: 0.0677 - val_accuracy: 0.9795\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0633 - accuracy: 0.9807 - val_loss: 0.0622 - val_accuracy: 0.9806\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0580 - accuracy: 0.9820 - val_loss: 0.0667 - val_accuracy: 0.9786\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0528 - accuracy: 0.9828 - val_loss: 0.0860 - val_accuracy: 0.9743\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0486 - accuracy: 0.9848 - val_loss: 0.0445 - val_accuracy: 0.9865\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0444 - accuracy: 0.9856 - val_loss: 0.0564 - val_accuracy: 0.9830\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0412 - accuracy: 0.9863 - val_loss: 0.0629 - val_accuracy: 0.9814\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0629 - accuracy: 0.9814\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.2925 - accuracy: 0.1209 - val_loss: 2.2691 - val_accuracy: 0.1575\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 2.0464 - accuracy: 0.2499 - val_loss: 1.4786 - val_accuracy: 0.4694\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 1.0199 - accuracy: 0.6608 - val_loss: 0.6647 - val_accuracy: 0.7948\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.6124 - accuracy: 0.8090 - val_loss: 0.5145 - val_accuracy: 0.8451\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.4739 - accuracy: 0.8539 - val_loss: 0.3880 - val_accuracy: 0.8864\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.4008 - accuracy: 0.8782 - val_loss: 0.3355 - val_accuracy: 0.8951\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.3490 - accuracy: 0.8936 - val_loss: 0.3040 - val_accuracy: 0.9073\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.3150 - accuracy: 0.9047 - val_loss: 0.2707 - val_accuracy: 0.9168\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.2861 - accuracy: 0.9133 - val_loss: 0.2709 - val_accuracy: 0.9161\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.2662 - accuracy: 0.9197 - val_loss: 0.2460 - val_accuracy: 0.9238\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2460 - accuracy: 0.9238\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 10s 7ms/step - loss: 0.4014 - accuracy: 0.8708 - val_loss: 0.1403 - val_accuracy: 0.9574\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.1367 - accuracy: 0.9586 - val_loss: 0.0945 - val_accuracy: 0.9689\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0998 - accuracy: 0.9693 - val_loss: 0.0711 - val_accuracy: 0.9779\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 0.0827 - accuracy: 0.9754 - val_loss: 0.0731 - val_accuracy: 0.9757\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0702 - accuracy: 0.9789 - val_loss: 0.0533 - val_accuracy: 0.9812\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0629 - accuracy: 0.9809 - val_loss: 0.0525 - val_accuracy: 0.9837\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 10s 8ms/step - loss: 0.0579 - accuracy: 0.9824 - val_loss: 0.0597 - val_accuracy: 0.9801\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0556 - accuracy: 0.9836 - val_loss: 0.0573 - val_accuracy: 0.9823\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0512 - accuracy: 0.9843 - val_loss: 0.0486 - val_accuracy: 0.9836\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.0484 - accuracy: 0.9856 - val_loss: 0.0425 - val_accuracy: 0.9868\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0425 - accuracy: 0.9868\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 5s 7ms/step - loss: 0.5000 - accuracy: 0.8395 - val_loss: 0.1812 - val_accuracy: 0.9437\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1533 - accuracy: 0.9532 - val_loss: 0.1333 - val_accuracy: 0.9577\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1108 - accuracy: 0.9664 - val_loss: 0.0938 - val_accuracy: 0.9700\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0909 - accuracy: 0.9718 - val_loss: 0.0922 - val_accuracy: 0.9718\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0771 - accuracy: 0.9763 - val_loss: 0.0703 - val_accuracy: 0.9772\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0670 - accuracy: 0.9791 - val_loss: 0.0622 - val_accuracy: 0.9806\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0639 - accuracy: 0.9804 - val_loss: 0.0617 - val_accuracy: 0.9797\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0578 - accuracy: 0.9815 - val_loss: 0.0604 - val_accuracy: 0.9800\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0521 - accuracy: 0.9841 - val_loss: 0.0564 - val_accuracy: 0.9822\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0515 - accuracy: 0.9834 - val_loss: 0.0518 - val_accuracy: 0.9834\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0518 - accuracy: 0.9834\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 5s 7ms/step - loss: 2.2942 - accuracy: 0.1679 - val_loss: 2.2863 - val_accuracy: 0.2103\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 2.2761 - accuracy: 0.2278 - val_loss: 2.2596 - val_accuracy: 0.2419\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 2.2275 - accuracy: 0.2374 - val_loss: 2.1764 - val_accuracy: 0.2278\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 2.0846 - accuracy: 0.2399 - val_loss: 1.9324 - val_accuracy: 0.3132\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 1.6778 - accuracy: 0.4390 - val_loss: 1.3860 - val_accuracy: 0.5720\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 1.2262 - accuracy: 0.6310 - val_loss: 1.0006 - val_accuracy: 0.7016\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.9198 - accuracy: 0.7146 - val_loss: 0.7730 - val_accuracy: 0.7616\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.7672 - accuracy: 0.7601 - val_loss: 0.7145 - val_accuracy: 0.7809\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.6808 - accuracy: 0.7871 - val_loss: 0.6014 - val_accuracy: 0.8196\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.6150 - accuracy: 0.8091 - val_loss: 0.5687 - val_accuracy: 0.8272\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.5687 - accuracy: 0.8272\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 8ms/step - loss: 0.4376 - accuracy: 0.8587 - val_loss: 0.1370 - val_accuracy: 0.9580\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.1367 - accuracy: 0.9585 - val_loss: 0.1039 - val_accuracy: 0.9682\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.1009 - accuracy: 0.9693 - val_loss: 0.0841 - val_accuracy: 0.9723\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0811 - accuracy: 0.9756 - val_loss: 0.0720 - val_accuracy: 0.9780\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0688 - accuracy: 0.9788 - val_loss: 0.0629 - val_accuracy: 0.9816\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.0575 - val_accuracy: 0.9815\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0541 - accuracy: 0.9829 - val_loss: 0.0554 - val_accuracy: 0.9831\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0495 - accuracy: 0.9850 - val_loss: 0.0502 - val_accuracy: 0.9840\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0453 - accuracy: 0.9860 - val_loss: 0.0588 - val_accuracy: 0.9813\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0428 - accuracy: 0.9869 - val_loss: 0.0500 - val_accuracy: 0.9839\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0500 - accuracy: 0.9839\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3031 - accuracy: 0.1089 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3028 - accuracy: 0.1094 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3028 - accuracy: 0.1092 - val_loss: 2.3031 - val_accuracy: 0.1010\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3027 - accuracy: 0.1086 - val_loss: 2.3033 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3028 - accuracy: 0.1088 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3028 - accuracy: 0.1091 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3029 - accuracy: 0.1093 - val_loss: 2.3030 - val_accuracy: 0.1010\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3028 - accuracy: 0.1066 - val_loss: 2.3024 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3026 - accuracy: 0.1103 - val_loss: 2.3029 - val_accuracy: 0.0958\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3028 - accuracy: 0.1098 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3018 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.6177 - accuracy: 0.7927 - val_loss: 0.1862 - val_accuracy: 0.9441\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1725 - accuracy: 0.9475 - val_loss: 0.1384 - val_accuracy: 0.9553\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1228 - accuracy: 0.9632 - val_loss: 0.1004 - val_accuracy: 0.9680\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.1005 - accuracy: 0.9691 - val_loss: 0.0766 - val_accuracy: 0.9758\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0858 - accuracy: 0.9737 - val_loss: 0.0731 - val_accuracy: 0.9762\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0755 - accuracy: 0.9768 - val_loss: 0.0758 - val_accuracy: 0.9744\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0680 - accuracy: 0.9795 - val_loss: 0.0626 - val_accuracy: 0.9795\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0610 - accuracy: 0.9811 - val_loss: 0.0557 - val_accuracy: 0.9824\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0560 - accuracy: 0.9826 - val_loss: 0.0642 - val_accuracy: 0.9782\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 0.0523 - accuracy: 0.9840 - val_loss: 0.0516 - val_accuracy: 0.9821\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0516 - accuracy: 0.9821\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 2.3028 - accuracy: 0.1088 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 2.3025 - accuracy: 0.1084 - val_loss: 2.3023 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 2.3026 - accuracy: 0.1110 - val_loss: 2.3028 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3027 - accuracy: 0.1087 - val_loss: 2.3031 - val_accuracy: 0.0982\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 2.3029 - accuracy: 0.1094 - val_loss: 2.3029 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3028 - accuracy: 0.1087 - val_loss: 2.3023 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 16s 7ms/step - loss: 2.3027 - accuracy: 0.1101 - val_loss: 2.3032 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3026 - accuracy: 0.1095 - val_loss: 2.3022 - val_accuracy: 0.1028\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3026 - accuracy: 0.1086 - val_loss: 2.3042 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3028 - accuracy: 0.1090 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3021 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.3439 - accuracy: 0.8937 - val_loss: 0.1967 - val_accuracy: 0.9411\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.2182 - accuracy: 0.9331 - val_loss: 0.1824 - val_accuracy: 0.9429\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.2445 - accuracy: 0.9250 - val_loss: 0.2739 - val_accuracy: 0.9124\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.4393 - accuracy: 0.8655 - val_loss: 0.2906 - val_accuracy: 0.9121\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 1.7938 - accuracy: 0.3179 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3024 - accuracy: 0.1101 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3023 - accuracy: 0.1095 - val_loss: 2.3020 - val_accuracy: 0.1028\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3023 - accuracy: 0.1097 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3022 - accuracy: 0.1114 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3024 - accuracy: 0.1102 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3016 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 0.5842 - accuracy: 0.8081 - val_loss: 0.2139 - val_accuracy: 0.9365\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1943 - accuracy: 0.9407 - val_loss: 0.1343 - val_accuracy: 0.9556\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 0.1385 - accuracy: 0.9578 - val_loss: 0.1216 - val_accuracy: 0.9615\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.1112 - accuracy: 0.9652 - val_loss: 0.1001 - val_accuracy: 0.9684\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.0960 - accuracy: 0.9704 - val_loss: 0.0923 - val_accuracy: 0.9706\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.0826 - accuracy: 0.9742 - val_loss: 0.0758 - val_accuracy: 0.9742\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.0759 - accuracy: 0.9764 - val_loss: 0.0667 - val_accuracy: 0.9792\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.0686 - accuracy: 0.9787 - val_loss: 0.0681 - val_accuracy: 0.9790\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.0643 - accuracy: 0.9801 - val_loss: 0.0599 - val_accuracy: 0.9809\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.0595 - accuracy: 0.9819 - val_loss: 0.0570 - val_accuracy: 0.9830\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0570 - accuracy: 0.9830\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 10s 7ms/step - loss: 2.3089 - accuracy: 0.1104 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3021 - accuracy: 0.1101 - val_loss: 2.3030 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3023 - accuracy: 0.1104 - val_loss: 2.3020 - val_accuracy: 0.1028\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3022 - accuracy: 0.1095 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3023 - accuracy: 0.1095 - val_loss: 2.3026 - val_accuracy: 0.0958\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3023 - accuracy: 0.1097 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3022 - accuracy: 0.1098 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3022 - accuracy: 0.1097 - val_loss: 2.3026 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3023 - accuracy: 0.1105 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3024 - accuracy: 0.1108 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3014 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 5s 7ms/step - loss: 0.3572 - accuracy: 0.8866 - val_loss: 0.1578 - val_accuracy: 0.9512\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1663 - accuracy: 0.9496 - val_loss: 0.1705 - val_accuracy: 0.9474\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.1528 - accuracy: 0.9534 - val_loss: 0.1549 - val_accuracy: 0.9560\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1511 - accuracy: 0.9550 - val_loss: 0.1126 - val_accuracy: 0.9653\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1578 - accuracy: 0.9519 - val_loss: 0.1507 - val_accuracy: 0.9555\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1674 - accuracy: 0.9488 - val_loss: 0.1957 - val_accuracy: 0.9392\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1708 - accuracy: 0.9480 - val_loss: 0.1273 - val_accuracy: 0.9607\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1651 - accuracy: 0.9501 - val_loss: 0.1310 - val_accuracy: 0.9613\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1604 - accuracy: 0.9513 - val_loss: 0.1689 - val_accuracy: 0.9497\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1926 - accuracy: 0.9406 - val_loss: 0.2402 - val_accuracy: 0.9275\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2402 - accuracy: 0.9275\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 5s 7ms/step - loss: 1.1091 - accuracy: 0.6132 - val_loss: 0.4340 - val_accuracy: 0.8597\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.3749 - accuracy: 0.8826 - val_loss: 0.2450 - val_accuracy: 0.9219\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.2464 - accuracy: 0.9243 - val_loss: 0.1811 - val_accuracy: 0.9438\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.2150 - accuracy: 0.9345 - val_loss: 0.1443 - val_accuracy: 0.9543\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1630 - accuracy: 0.9503 - val_loss: 0.1508 - val_accuracy: 0.9533\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 0.1394 - accuracy: 0.9567 - val_loss: 0.1375 - val_accuracy: 0.9574\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1275 - accuracy: 0.9600 - val_loss: 0.1041 - val_accuracy: 0.9673\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1170 - accuracy: 0.9641 - val_loss: 0.0904 - val_accuracy: 0.9709\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1067 - accuracy: 0.9670 - val_loss: 0.1002 - val_accuracy: 0.9686\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1003 - accuracy: 0.9698 - val_loss: 0.1023 - val_accuracy: 0.9680\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1023 - accuracy: 0.9680\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 8ms/step - loss: 2.3033 - accuracy: 0.1113 - val_loss: 2.3021 - val_accuracy: 0.1028\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3018 - accuracy: 0.1104 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3020 - accuracy: 0.1112 - val_loss: 2.3018 - val_accuracy: 0.1028\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3019 - accuracy: 0.1107 - val_loss: 2.3025 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3020 - accuracy: 0.1103 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3018 - accuracy: 0.1097 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3018 - accuracy: 0.1118 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3020 - accuracy: 0.1104 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3018 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1032\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3019 - accuracy: 0.1116 - val_loss: 2.3026 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3026 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 3.3651 - accuracy: 0.1019 - val_loss: 2.3099 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3154 - accuracy: 0.1046 - val_loss: 2.3156 - val_accuracy: 0.1028\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3142 - accuracy: 0.1010 - val_loss: 2.3103 - val_accuracy: 0.0980\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3158 - accuracy: 0.1017 - val_loss: 2.3048 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3164 - accuracy: 0.1012 - val_loss: 2.3111 - val_accuracy: 0.1010\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3167 - accuracy: 0.1031 - val_loss: 2.3324 - val_accuracy: 0.1032\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3159 - accuracy: 0.1049 - val_loss: 2.3155 - val_accuracy: 0.1009\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3157 - accuracy: 0.1046 - val_loss: 2.3106 - val_accuracy: 0.0892\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3159 - accuracy: 0.1043 - val_loss: 2.3070 - val_accuracy: 0.1009\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3156 - accuracy: 0.1014 - val_loss: 2.3078 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3078 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 14s 6ms/step - loss: 2.3023 - accuracy: 0.1098 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 15s 6ms/step - loss: 2.3022 - accuracy: 0.1110 - val_loss: 2.3018 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3021 - accuracy: 0.1102 - val_loss: 2.3022 - val_accuracy: 0.0982\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3023 - accuracy: 0.1109 - val_loss: 2.3014 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3020 - accuracy: 0.1103 - val_loss: 2.3019 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3023 - accuracy: 0.1109 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3021 - accuracy: 0.1101 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3022 - accuracy: 0.1105 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 13s 5ms/step - loss: 2.3020 - accuracy: 0.1103 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 13s 6ms/step - loss: 2.3021 - accuracy: 0.1106 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3014 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "2400/2400 [==============================] - 18s 7ms/step - loss: 91736.6719 - accuracy: 0.1021 - val_loss: 2.3117 - val_accuracy: 0.1032\n",
            "Epoch 2/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3165 - accuracy: 0.1041 - val_loss: 2.3098 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3170 - accuracy: 0.1017 - val_loss: 2.3183 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3165 - accuracy: 0.1008 - val_loss: 2.3157 - val_accuracy: 0.0892\n",
            "Epoch 5/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3166 - accuracy: 0.1045 - val_loss: 2.3065 - val_accuracy: 0.1010\n",
            "Epoch 6/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3167 - accuracy: 0.1039 - val_loss: 2.3136 - val_accuracy: 0.1028\n",
            "Epoch 7/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3168 - accuracy: 0.1025 - val_loss: 2.3059 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "2400/2400 [==============================] - 18s 8ms/step - loss: 2.3168 - accuracy: 0.1020 - val_loss: 2.3100 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3155 - accuracy: 0.1034 - val_loss: 2.3190 - val_accuracy: 0.1028\n",
            "Epoch 10/10\n",
            "2400/2400 [==============================] - 17s 7ms/step - loss: 2.3168 - accuracy: 0.1006 - val_loss: 2.3113 - val_accuracy: 0.0974\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3113 - accuracy: 0.0974\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 8s 6ms/step - loss: 7.3917 - accuracy: 0.1037 - val_loss: 2.3123 - val_accuracy: 0.0982\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3108 - accuracy: 0.1046 - val_loss: 2.3073 - val_accuracy: 0.0974\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3112 - accuracy: 0.1033 - val_loss: 2.3069 - val_accuracy: 0.0980\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3113 - accuracy: 0.1030 - val_loss: 2.3125 - val_accuracy: 0.1010\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3113 - accuracy: 0.1036 - val_loss: 2.3079 - val_accuracy: 0.1010\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3113 - accuracy: 0.1049 - val_loss: 2.3054 - val_accuracy: 0.1010\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3107 - accuracy: 0.1026 - val_loss: 2.3108 - val_accuracy: 0.1010\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3121 - accuracy: 0.1032 - val_loss: 2.3091 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3102 - accuracy: 0.1057 - val_loss: 2.3038 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 2.3111 - accuracy: 0.1037 - val_loss: 2.3117 - val_accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3117 - accuracy: 0.1135\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.8163 - accuracy: 0.7181 - val_loss: 0.1702 - val_accuracy: 0.9487\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1537 - accuracy: 0.9539 - val_loss: 0.5795 - val_accuracy: 0.8405\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.1373 - accuracy: 0.9581 - val_loss: 0.0835 - val_accuracy: 0.9720\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0867 - accuracy: 0.9736 - val_loss: 0.0712 - val_accuracy: 0.9782\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 6s 5ms/step - loss: 0.0733 - accuracy: 0.9777 - val_loss: 0.0641 - val_accuracy: 0.9806\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0637 - accuracy: 0.9795 - val_loss: 0.0624 - val_accuracy: 0.9818\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0597 - accuracy: 0.9814 - val_loss: 0.0637 - val_accuracy: 0.9798\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0551 - accuracy: 0.9826 - val_loss: 0.0538 - val_accuracy: 0.9817\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 7s 6ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.0529 - val_accuracy: 0.9834\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 7s 5ms/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.0651 - val_accuracy: 0.9807\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9807\n",
            "Epoch 1/10\n",
            "1200/1200 [==============================] - 10s 7ms/step - loss: 528117.9375 - accuracy: 0.1045 - val_loss: 2.3067 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3124 - accuracy: 0.1018 - val_loss: 2.3063 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "1200/1200 [==============================] - 10s 8ms/step - loss: 2.3118 - accuracy: 0.1041 - val_loss: 2.3035 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3123 - accuracy: 0.1023 - val_loss: 2.3198 - val_accuracy: 0.1009\n",
            "Epoch 5/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3112 - accuracy: 0.1056 - val_loss: 2.3079 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3120 - accuracy: 0.1046 - val_loss: 2.3092 - val_accuracy: 0.1010\n",
            "Epoch 7/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3124 - accuracy: 0.1037 - val_loss: 2.3075 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "1200/1200 [==============================] - 8s 7ms/step - loss: 2.3118 - accuracy: 0.1031 - val_loss: 2.3083 - val_accuracy: 0.1010\n",
            "Epoch 9/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3121 - accuracy: 0.1047 - val_loss: 2.3069 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "1200/1200 [==============================] - 9s 7ms/step - loss: 2.3125 - accuracy: 0.1034 - val_loss: 2.3060 - val_accuracy: 0.1028\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3060 - accuracy: 0.1028\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 5s 7ms/step - loss: 2.3082 - accuracy: 0.1034 - val_loss: 2.3080 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3084 - accuracy: 0.1025 - val_loss: 2.3075 - val_accuracy: 0.0892\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 2.3081 - accuracy: 0.1059 - val_loss: 2.3068 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3081 - accuracy: 0.1059 - val_loss: 2.3190 - val_accuracy: 0.1028\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3085 - accuracy: 0.1032 - val_loss: 2.3140 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3086 - accuracy: 0.1049 - val_loss: 2.3097 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3087 - accuracy: 0.1035 - val_loss: 2.3084 - val_accuracy: 0.1010\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3079 - accuracy: 0.1045 - val_loss: 2.3084 - val_accuracy: 0.0974\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3082 - accuracy: 0.1024 - val_loss: 2.3064 - val_accuracy: 0.1032\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3076 - accuracy: 0.1052 - val_loss: 2.3076 - val_accuracy: 0.0980\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3076 - accuracy: 0.0980\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 5s 7ms/step - loss: 1.4009 - accuracy: 0.4934 - val_loss: 0.3053 - val_accuracy: 0.9024\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.2561 - accuracy: 0.9206 - val_loss: 0.1827 - val_accuracy: 0.9448\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1434 - accuracy: 0.9574 - val_loss: 0.0969 - val_accuracy: 0.9708\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.1006 - accuracy: 0.9693 - val_loss: 0.0862 - val_accuracy: 0.9762\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0852 - accuracy: 0.9746 - val_loss: 0.0699 - val_accuracy: 0.9786\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 0.0723 - accuracy: 0.9786 - val_loss: 0.0792 - val_accuracy: 0.9762\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0650 - accuracy: 0.9807 - val_loss: 0.0782 - val_accuracy: 0.9746\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0580 - accuracy: 0.9826 - val_loss: 0.0707 - val_accuracy: 0.9793\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0546 - accuracy: 0.9837 - val_loss: 0.0672 - val_accuracy: 0.9799\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0491 - accuracy: 0.9846 - val_loss: 0.0639 - val_accuracy: 0.9812\n",
            "313/313 [==============================] - 2s 3ms/step - loss: 0.0639 - accuracy: 0.9812\n",
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 8ms/step - loss: 675409.1250 - accuracy: 0.1036 - val_loss: 2.3059 - val_accuracy: 0.0974\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3093 - accuracy: 0.1036 - val_loss: 2.3122 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3091 - accuracy: 0.1043 - val_loss: 2.3140 - val_accuracy: 0.1010\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3090 - accuracy: 0.1043 - val_loss: 2.3066 - val_accuracy: 0.1009\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3090 - accuracy: 0.1034 - val_loss: 2.3095 - val_accuracy: 0.0958\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 2.3095 - accuracy: 0.1018 - val_loss: 2.3068 - val_accuracy: 0.0892\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3087 - accuracy: 0.1029 - val_loss: 2.3125 - val_accuracy: 0.0892\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3090 - accuracy: 0.1051 - val_loss: 2.3050 - val_accuracy: 0.1032\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 4s 7ms/step - loss: 2.3094 - accuracy: 0.1041 - val_loss: 2.3054 - val_accuracy: 0.1028\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 2.3085 - accuracy: 0.1052 - val_loss: 2.3125 - val_accuracy: 0.0892\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.3125 - accuracy: 0.0892\n"
          ]
        }
      ],
      "source": [
        "a = []\n",
        "d = []\n",
        "c = []\n",
        "e = []\n",
        "\n",
        "# define the model for the convolutional neural net for reducing filters\n",
        "for lr in learning_rate:\n",
        "    for bs in batch_size:\n",
        "        for oc in optimizer_choice:\n",
        "            # define the model for the convolutional neural net for HOURGLASS filters\n",
        "            # Training the layers for the CNN using Keras\n",
        "            # The three convolutional layers\n",
        "            model = tf.keras.models.Sequential()\n",
        "            model.add(tf.keras.layers.Conv2D(2, (3, 3), kernel_initializer='he_uniform', activation='relu', input_shape=(28, 28, 1)))\n",
        "            model.add(tf.keras.layers.Conv2D(4, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(8, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(24, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(24, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(16, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(8, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.Conv2D(4, (3, 3), kernel_initializer='he_uniform', activation='relu'))\n",
        "            model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "            # Flatten the convulational layers and add the dense layers with 84 neurons and the relu activation\n",
        "            # and the output layer with the softmax output layer with 10 nominal output.\n",
        "            model.add(tf.keras.layers.Flatten())\n",
        "            model.add(tf.keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform')) # layer 9\n",
        "            model.add(tf.keras.layers.Dense(10, activation='softmax')) # layer 10\n",
        "\n",
        "            # Compile\n",
        "            opt = oc(learning_rate = lr)\n",
        "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "            # Fit the training data to the model\n",
        "            NNmodel = model.fit(x_train, y_train, batch_size=bs, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "            # lists\n",
        "            a.append(lr)\n",
        "            d.append(bs)\n",
        "            c.append(oc)\n",
        "            e.append(model.evaluate(x_test, y_test)[1])\n",
        "\n",
        "models3 = pd.DataFrame({'Learning Rate': a,\n",
        "                        'Batch Size': d,\n",
        "                        'Optimizer': c,\n",
        "                        'Accuracy': e})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "8xea8hqu3mQ8",
        "outputId": "56664de6-b1ca-4520-a6ef-725869dff081"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Learning Rate  Batch Size  \\\n",
              "5           0.001          50   \n",
              "8           0.001         100   \n",
              "0           0.001          25   \n",
              "6           0.001         100   \n",
              "13          0.010          50   \n",
              "2           0.001          25   \n",
              "10          0.010          25   \n",
              "3           0.001          50   \n",
              "25          0.100         100   \n",
              "22          0.100          50   \n",
              "1           0.001          25   \n",
              "16          0.010         100   \n",
              "15          0.010         100   \n",
              "4           0.001          50   \n",
              "7           0.001         100   \n",
              "14          0.010          50   \n",
              "12          0.010          50   \n",
              "11          0.010          25   \n",
              "17          0.010         100   \n",
              "18          0.100          25   \n",
              "19          0.100          25   \n",
              "21          0.100          50   \n",
              "9           0.010          25   \n",
              "23          0.100          50   \n",
              "24          0.100         100   \n",
              "20          0.100          25   \n",
              "26          0.100         100   \n",
              "\n",
              "                                            Optimizer  Accuracy  \n",
              "5   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9868  \n",
              "8   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9839  \n",
              "0   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9834  \n",
              "6   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9834  \n",
              "13  <class 'keras.optimizers.optimizer_v2.gradient...    0.9830  \n",
              "2   <class 'keras.optimizers.optimizer_v2.rmsprop....    0.9828  \n",
              "10  <class 'keras.optimizers.optimizer_v2.gradient...    0.9821  \n",
              "3   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9814  \n",
              "25  <class 'keras.optimizers.optimizer_v2.gradient...    0.9812  \n",
              "22  <class 'keras.optimizers.optimizer_v2.gradient...    0.9807  \n",
              "1   <class 'keras.optimizers.optimizer_v2.gradient...    0.9689  \n",
              "16  <class 'keras.optimizers.optimizer_v2.gradient...    0.9680  \n",
              "15  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.9275  \n",
              "4   <class 'keras.optimizers.optimizer_v2.gradient...    0.9238  \n",
              "7   <class 'keras.optimizers.optimizer_v2.gradient...    0.8272  \n",
              "14  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "12  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "11  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "17  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1135  \n",
              "18  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "19  <class 'keras.optimizers.optimizer_v2.gradient...    0.1135  \n",
              "21  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "9   <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.1135  \n",
              "23  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.1028  \n",
              "24  <class 'keras.optimizers.optimizer_v2.adam.Adam'>    0.0980  \n",
              "20  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.0974  \n",
              "26  <class 'keras.optimizers.optimizer_v2.rmsprop....    0.0892  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab4c74c3-8942-43b3-85da-227ee930e14d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.9828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.9275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.9238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.8272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.010</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.010</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.gradient...</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.010</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.100</td>\n",
              "      <td>50</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.adam.Adam'&gt;</td>\n",
              "      <td>0.0980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.100</td>\n",
              "      <td>25</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.0974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.100</td>\n",
              "      <td>100</td>\n",
              "      <td>&lt;class 'keras.optimizers.optimizer_v2.rmsprop....</td>\n",
              "      <td>0.0892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab4c74c3-8942-43b3-85da-227ee930e14d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab4c74c3-8942-43b3-85da-227ee930e14d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab4c74c3-8942-43b3-85da-227ee930e14d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "models3.sort_values('Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fu-63WnBol_"
      },
      "source": [
        "The CNNs were trained on MNIST dataset with the 10 convulutional layers using the decreasing, increasing and hourglass convulutional structures for the layers. These different neural networks were trained on mulitple parameters and iterated through learning rates of between 0.001, 0.01 and 0.1, batch sizes of 25, 50 and 100. The optimizers between tf.keras.optimizers.Adam, tf.keras.optimizers.SGD and tf.keras.optimizers.RMSprop were also iterated through. The accuracy is the 2nd value in the tables. The code somehow took too long to run so I could not clean that up. These models were passed through all the models. The first model performed the most on parameters - learning rate of 0.001, batch size of 100 and with the optimizer Adam. The second model went through the same parameters and the most accurate had learning rate of 0.001, batch size of 50 and the Adam optimizer. The funnel network performed best with the learning rate of 0.001, batch size of 50 and the RMS prop optimzer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSrlI4B808wP"
      },
      "source": [
        "# Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTfca4Z708wP",
        "outputId": "ee3e0560-9257-486b-a600-ddef38867627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 14s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Obtaining the CIFAR-10 dataset for the implementation for both training and testing data using built-in functions\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "x_train = (x_train.astype('float32')) / 255.0\n",
        "x_test = (x_test.astype('float32')) / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j82itrEZ08wQ",
        "outputId": "99907020-d800-4644-f546-c4440f26ac83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "782/782 [==============================] - 5s 5ms/step - loss: 1.9753 - accuracy: 0.2866 - val_loss: 1.7642 - val_accuracy: 0.3766\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.6941 - accuracy: 0.3994 - val_loss: 1.6326 - val_accuracy: 0.4184\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.5843 - accuracy: 0.4365 - val_loss: 1.5571 - val_accuracy: 0.4413\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.5199 - accuracy: 0.4567 - val_loss: 1.5019 - val_accuracy: 0.4554\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4734 - accuracy: 0.4758 - val_loss: 1.4699 - val_accuracy: 0.4688\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 1.4345 - accuracy: 0.4860 - val_loss: 1.4305 - val_accuracy: 0.4837\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4033 - accuracy: 0.4984 - val_loss: 1.4161 - val_accuracy: 0.4908\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3751 - accuracy: 0.5099 - val_loss: 1.3773 - val_accuracy: 0.5012\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3496 - accuracy: 0.5195 - val_loss: 1.3647 - val_accuracy: 0.5119\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.3283 - accuracy: 0.5262 - val_loss: 1.3424 - val_accuracy: 0.5171\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3069 - accuracy: 0.5361 - val_loss: 1.3228 - val_accuracy: 0.5252\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2896 - accuracy: 0.5422 - val_loss: 1.3078 - val_accuracy: 0.5297\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2718 - accuracy: 0.5495 - val_loss: 1.3106 - val_accuracy: 0.5252\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2557 - accuracy: 0.5550 - val_loss: 1.2844 - val_accuracy: 0.5405\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2417 - accuracy: 0.5598 - val_loss: 1.2687 - val_accuracy: 0.5441\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2265 - accuracy: 0.5669 - val_loss: 1.2756 - val_accuracy: 0.5423\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2135 - accuracy: 0.5721 - val_loss: 1.2575 - val_accuracy: 0.5543\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.2004 - accuracy: 0.5782 - val_loss: 1.2513 - val_accuracy: 0.5484\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1884 - accuracy: 0.5814 - val_loss: 1.2321 - val_accuracy: 0.5595\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1763 - accuracy: 0.5875 - val_loss: 1.2362 - val_accuracy: 0.5629\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1666 - accuracy: 0.5912 - val_loss: 1.2241 - val_accuracy: 0.5647\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1549 - accuracy: 0.5954 - val_loss: 1.2230 - val_accuracy: 0.5645\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1477 - accuracy: 0.5974 - val_loss: 1.2062 - val_accuracy: 0.5726\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1359 - accuracy: 0.6010 - val_loss: 1.1985 - val_accuracy: 0.5750\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.1258 - accuracy: 0.6052 - val_loss: 1.1948 - val_accuracy: 0.5796\n",
            "Epoch 1/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.6608 - accuracy: 0.3918 - val_loss: 1.4465 - val_accuracy: 0.4770\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3881 - accuracy: 0.4996 - val_loss: 1.3420 - val_accuracy: 0.5116\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2781 - accuracy: 0.5423 - val_loss: 1.3109 - val_accuracy: 0.5336\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2028 - accuracy: 0.5709 - val_loss: 1.2774 - val_accuracy: 0.5500\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 1.1395 - accuracy: 0.5948 - val_loss: 1.2092 - val_accuracy: 0.5655\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 1.0858 - accuracy: 0.6135 - val_loss: 1.1898 - val_accuracy: 0.5826\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 1.0418 - accuracy: 0.6313 - val_loss: 1.1502 - val_accuracy: 0.5955\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.0047 - accuracy: 0.6431 - val_loss: 1.1616 - val_accuracy: 0.5941\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.9688 - accuracy: 0.6566 - val_loss: 1.1814 - val_accuracy: 0.5920\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.9308 - accuracy: 0.6721 - val_loss: 1.1537 - val_accuracy: 0.5984\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.9039 - accuracy: 0.6788 - val_loss: 1.1613 - val_accuracy: 0.5985\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.8743 - accuracy: 0.6915 - val_loss: 1.1895 - val_accuracy: 0.6003\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.8446 - accuracy: 0.6984 - val_loss: 1.1842 - val_accuracy: 0.6017\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.8181 - accuracy: 0.7112 - val_loss: 1.1958 - val_accuracy: 0.6048\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7914 - accuracy: 0.7223 - val_loss: 1.2053 - val_accuracy: 0.6014\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7664 - accuracy: 0.7281 - val_loss: 1.2107 - val_accuracy: 0.5972\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7473 - accuracy: 0.7337 - val_loss: 1.3191 - val_accuracy: 0.5891\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7257 - accuracy: 0.7419 - val_loss: 1.2444 - val_accuracy: 0.5981\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.7057 - accuracy: 0.7473 - val_loss: 1.2390 - val_accuracy: 0.6056\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.6803 - accuracy: 0.7573 - val_loss: 1.2959 - val_accuracy: 0.6044\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6630 - accuracy: 0.7640 - val_loss: 1.2980 - val_accuracy: 0.6025\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6487 - accuracy: 0.7689 - val_loss: 1.3492 - val_accuracy: 0.5905\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6269 - accuracy: 0.7752 - val_loss: 1.3935 - val_accuracy: 0.5887\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.6173 - accuracy: 0.7777 - val_loss: 1.4210 - val_accuracy: 0.5886\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.5945 - accuracy: 0.7875 - val_loss: 1.4238 - val_accuracy: 0.5869\n",
            "Epoch 1/25\n",
            "782/782 [==============================] - 5s 5ms/step - loss: 1.8234 - accuracy: 0.3410 - val_loss: 1.6889 - val_accuracy: 0.3967\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.6482 - accuracy: 0.4098 - val_loss: 1.6999 - val_accuracy: 0.3871\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.6014 - accuracy: 0.4264 - val_loss: 1.5875 - val_accuracy: 0.4371\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.5528 - accuracy: 0.4461 - val_loss: 1.5525 - val_accuracy: 0.4455\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.5232 - accuracy: 0.4573 - val_loss: 1.6095 - val_accuracy: 0.4383\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 1.4995 - accuracy: 0.4645 - val_loss: 1.5658 - val_accuracy: 0.4548\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4780 - accuracy: 0.4752 - val_loss: 1.4961 - val_accuracy: 0.4712\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4506 - accuracy: 0.4845 - val_loss: 1.4830 - val_accuracy: 0.4785\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4452 - accuracy: 0.4897 - val_loss: 1.4790 - val_accuracy: 0.4732\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4344 - accuracy: 0.4896 - val_loss: 1.5261 - val_accuracy: 0.4637\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4077 - accuracy: 0.5008 - val_loss: 1.5382 - val_accuracy: 0.4550\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3939 - accuracy: 0.5054 - val_loss: 1.5106 - val_accuracy: 0.4770\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3807 - accuracy: 0.5118 - val_loss: 1.5852 - val_accuracy: 0.4516\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3730 - accuracy: 0.5159 - val_loss: 1.4523 - val_accuracy: 0.4927\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3752 - accuracy: 0.5165 - val_loss: 1.5004 - val_accuracy: 0.4873\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3547 - accuracy: 0.5243 - val_loss: 1.5300 - val_accuracy: 0.4729\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3431 - accuracy: 0.5260 - val_loss: 1.5078 - val_accuracy: 0.4860\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3349 - accuracy: 0.5298 - val_loss: 1.4957 - val_accuracy: 0.4820\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3262 - accuracy: 0.5353 - val_loss: 1.4606 - val_accuracy: 0.4936\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3100 - accuracy: 0.5396 - val_loss: 1.5235 - val_accuracy: 0.4969\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3103 - accuracy: 0.5406 - val_loss: 1.4693 - val_accuracy: 0.4860\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3112 - accuracy: 0.5405 - val_loss: 1.5439 - val_accuracy: 0.4744\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3114 - accuracy: 0.5432 - val_loss: 1.4667 - val_accuracy: 0.4914\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.2967 - accuracy: 0.5453 - val_loss: 1.4640 - val_accuracy: 0.4992\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.3053 - accuracy: 0.5442 - val_loss: 1.4639 - val_accuracy: 0.4951\n",
            "Epoch 1/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3193 - accuracy: 0.0985 - val_loss: 2.3091 - val_accuracy: 0.1000\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3113 - accuracy: 0.1030 - val_loss: 2.3103 - val_accuracy: 0.1000\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3115 - accuracy: 0.0989 - val_loss: 2.3140 - val_accuracy: 0.1000\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3119 - accuracy: 0.0997 - val_loss: 2.3068 - val_accuracy: 0.1000\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3112 - accuracy: 0.1030 - val_loss: 2.3074 - val_accuracy: 0.1000\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3121 - accuracy: 0.1000 - val_loss: 2.3171 - val_accuracy: 0.1000\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3122 - accuracy: 0.1008 - val_loss: 2.3113 - val_accuracy: 0.1000\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3118 - accuracy: 0.1004 - val_loss: 2.3085 - val_accuracy: 0.1000\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3125 - accuracy: 0.0992 - val_loss: 2.3097 - val_accuracy: 0.1000\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3110 - accuracy: 0.0995 - val_loss: 2.3060 - val_accuracy: 0.1000\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3112 - accuracy: 0.0993 - val_loss: 2.3088 - val_accuracy: 0.1000\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3102 - accuracy: 0.1025 - val_loss: 2.3158 - val_accuracy: 0.1000\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3118 - accuracy: 0.1007 - val_loss: 2.3109 - val_accuracy: 0.1000\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3124 - accuracy: 0.0976 - val_loss: 2.3062 - val_accuracy: 0.1000\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3111 - accuracy: 0.0997 - val_loss: 2.3137 - val_accuracy: 0.1000\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3115 - accuracy: 0.0985 - val_loss: 2.3123 - val_accuracy: 0.1000\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3111 - accuracy: 0.0971 - val_loss: 2.3120 - val_accuracy: 0.1000\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3121 - accuracy: 0.0984 - val_loss: 2.3098 - val_accuracy: 0.1000\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3109 - accuracy: 0.1049 - val_loss: 2.3096 - val_accuracy: 0.1000\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3116 - accuracy: 0.0991 - val_loss: 2.3068 - val_accuracy: 0.1000\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3113 - accuracy: 0.0987 - val_loss: 2.3127 - val_accuracy: 0.1000\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3104 - accuracy: 0.0997 - val_loss: 2.3189 - val_accuracy: 0.1000\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3120 - accuracy: 0.1002 - val_loss: 2.3186 - val_accuracy: 0.1000\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3114 - accuracy: 0.1022 - val_loss: 2.3098 - val_accuracy: 0.1000\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.3119 - accuracy: 0.0987 - val_loss: 2.3124 - val_accuracy: 0.1000\n"
          ]
        }
      ],
      "source": [
        "# define empty lists to store the accuracy and loss values for model comparison\n",
        "loss = []\n",
        "lossv = []\n",
        "accuracy = []\n",
        "accuracyv = []\n",
        "learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "for lr in learning_rate:\n",
        "    # Training the layers for the CNN using Keras\n",
        "    # The three convolutional layers\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(tf.keras.layers.Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(tf.keras.layers.Conv2D(120, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
        "    \n",
        "    # Flatten the convulational layers and add the dense layers with 84 neurons and the relu activation\n",
        "    # and the output layer with the softmax output layer with 10 nominal output.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = lr)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    # Fit the training data to the model\n",
        "    NNmodel = model.fit(x_train, y_train, batch_size=64, epochs=25, validation_data=(x_test, y_test))\n",
        "\n",
        "    accuracyv.append((NNmodel.history['val_accuracy'])[-1])\n",
        "    accuracy.append((NNmodel.history['accuracy'])[-1])\n",
        "    lossv.append((NNmodel.history['val_loss'])[-1])\n",
        "    loss.append((NNmodel.history['loss'])[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "yp1ZdpkD08wS",
        "outputId": "dd9e04ee-2811-437d-c542-9e8b0e16e83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# learning rate against loss plot\n",
        "# using a log plot since the learning rate values are far apart and it looks weird.\n",
        "plt.scatter(learning_rate, loss, color ='red') \n",
        "plt.plot(learning_rate, loss, color ='red') \n",
        "plt.legend() \n",
        "plt.title(\"Learning rate against loss\", fontsize=15) \n",
        "plt.ylabel('Loss', fontsize = 12) \n",
        "plt.xlabel('Learning rate', fontsize = 12) \n",
        "plt.xscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "Ik6CxzR008wT",
        "outputId": "87f87197-d962-455f-cdc0-7e05b643407c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# learning rate against accuracy plot\n",
        "plt.scatter(learning_rate, accuracy, color ='blue') \n",
        "plt.plot(learning_rate, accuracy, color ='blue') \n",
        "plt.legend() \n",
        "plt.title(\"Learning rate against accuracy\", fontsize=15) \n",
        "plt.ylabel('Accuracy', fontsize = 12) \n",
        "plt.xlabel('Learning rate', fontsize = 12) \n",
        "plt.xscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "mlsJNMxX58ph",
        "outputId": "2c732faa-7448-4607-86a1-f714b836aa41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# learning rate against loss plot\n",
        "# using a log plot since the learning rate values are far apart and it looks weird.\n",
        "plt.scatter(learning_rate, loss, color ='red') \n",
        "plt.plot(learning_rate, loss, color ='red') \n",
        "plt.legend() \n",
        "plt.title(\"Learning rate against validation loss\", fontsize=15) \n",
        "plt.ylabel('Loss', fontsize = 12) \n",
        "plt.xlabel('Learning rate', fontsize = 12) \n",
        "plt.xscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "KOgl0mzK860S",
        "outputId": "44ed66f4-98f5-48fe-d957-25d0a69c5acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# learning rate against accuracy plot\n",
        "plt.scatter(learning_rate, accuracyv, color ='blue') \n",
        "plt.plot(learning_rate, accuracyv, color ='blue') \n",
        "plt.legend() \n",
        "plt.title(\"Learning rate against validation accuracy\", fontsize=15) \n",
        "plt.ylabel('Accuracy', fontsize = 12) \n",
        "plt.xlabel('Learning rate', fontsize = 12) \n",
        "plt.xscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB09mAlq5C0d"
      },
      "source": [
        "1. The effect of the learning was tested with 4 different learning rates. 0.0001, 0.001, 0.01 and 0.1. Increasing the learning rate from 0.0001 to 0.001 reduced the loss of the training and increased the accuracy of the training process. However, a further increase in the learning rate threw the model completely off. The loss rose to about 2.25 and the accuracy dropped to 0.1 which is extremely bad. The best performing learning rate was 0.001 with the lowest loss and highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPumcmZ008wU",
        "outputId": "4dcbf1d5-ebf0-4e2f-ffcd-764c9744f701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 1.5903 - accuracy: 0.4229 - val_loss: 1.3552 - val_accuracy: 0.5177\n",
            "Epoch 2/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.2955 - accuracy: 0.5413 - val_loss: 1.2662 - val_accuracy: 0.5469\n",
            "Epoch 3/25\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 1.1839 - accuracy: 0.5808 - val_loss: 1.1761 - val_accuracy: 0.5861\n",
            "Epoch 4/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.1108 - accuracy: 0.6090 - val_loss: 1.1948 - val_accuracy: 0.5767\n",
            "Epoch 5/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.0463 - accuracy: 0.6302 - val_loss: 1.1433 - val_accuracy: 0.5973\n",
            "Epoch 6/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.9983 - accuracy: 0.6460 - val_loss: 1.1089 - val_accuracy: 0.6185\n",
            "Epoch 7/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.9488 - accuracy: 0.6660 - val_loss: 1.0932 - val_accuracy: 0.6312\n",
            "Epoch 8/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.9117 - accuracy: 0.6790 - val_loss: 1.1397 - val_accuracy: 0.6152\n",
            "Epoch 9/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8724 - accuracy: 0.6913 - val_loss: 1.1480 - val_accuracy: 0.6118\n",
            "Epoch 10/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.8366 - accuracy: 0.7054 - val_loss: 1.1319 - val_accuracy: 0.6152\n",
            "Epoch 11/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.8087 - accuracy: 0.7143 - val_loss: 1.1367 - val_accuracy: 0.6213\n",
            "Epoch 12/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.7764 - accuracy: 0.7247 - val_loss: 1.1374 - val_accuracy: 0.6230\n",
            "Epoch 13/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.7499 - accuracy: 0.7352 - val_loss: 1.1753 - val_accuracy: 0.6155\n",
            "Epoch 14/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.7213 - accuracy: 0.7443 - val_loss: 1.2152 - val_accuracy: 0.6151\n",
            "Epoch 15/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.6977 - accuracy: 0.7533 - val_loss: 1.1812 - val_accuracy: 0.6252\n",
            "Epoch 16/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.6791 - accuracy: 0.7574 - val_loss: 1.2285 - val_accuracy: 0.6265\n",
            "Epoch 17/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.6557 - accuracy: 0.7659 - val_loss: 1.2904 - val_accuracy: 0.6117\n",
            "Epoch 18/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.6369 - accuracy: 0.7742 - val_loss: 1.2809 - val_accuracy: 0.6085\n",
            "Epoch 19/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.6158 - accuracy: 0.7809 - val_loss: 1.2785 - val_accuracy: 0.6187\n",
            "Epoch 20/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5945 - accuracy: 0.7872 - val_loss: 1.3605 - val_accuracy: 0.6048\n",
            "Epoch 21/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.5817 - accuracy: 0.7926 - val_loss: 1.3814 - val_accuracy: 0.6093\n",
            "Epoch 22/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.5610 - accuracy: 0.8007 - val_loss: 1.4396 - val_accuracy: 0.6068\n",
            "Epoch 23/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5408 - accuracy: 0.8067 - val_loss: 1.5483 - val_accuracy: 0.5921\n",
            "Epoch 24/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.5340 - accuracy: 0.8087 - val_loss: 1.4755 - val_accuracy: 0.6073\n",
            "Epoch 25/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.5161 - accuracy: 0.8157 - val_loss: 1.5093 - val_accuracy: 0.5901\n",
            "Epoch 1/25\n",
            "1000/1000 [==============================] - 5s 4ms/step - loss: 1.6332 - accuracy: 0.4079 - val_loss: 1.3889 - val_accuracy: 0.5066\n",
            "Epoch 2/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3334 - accuracy: 0.5246 - val_loss: 1.2884 - val_accuracy: 0.5396\n",
            "Epoch 3/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2209 - accuracy: 0.5673 - val_loss: 1.2113 - val_accuracy: 0.5703\n",
            "Epoch 4/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1444 - accuracy: 0.5974 - val_loss: 1.1824 - val_accuracy: 0.5843\n",
            "Epoch 5/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0899 - accuracy: 0.6159 - val_loss: 1.1780 - val_accuracy: 0.5886\n",
            "Epoch 6/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0369 - accuracy: 0.6368 - val_loss: 1.1242 - val_accuracy: 0.6046\n",
            "Epoch 7/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9955 - accuracy: 0.6507 - val_loss: 1.0905 - val_accuracy: 0.6218\n",
            "Epoch 8/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9565 - accuracy: 0.6643 - val_loss: 1.1077 - val_accuracy: 0.6181\n",
            "Epoch 9/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9210 - accuracy: 0.6777 - val_loss: 1.0932 - val_accuracy: 0.6187\n",
            "Epoch 10/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8850 - accuracy: 0.6897 - val_loss: 1.1474 - val_accuracy: 0.6092\n",
            "Epoch 11/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8549 - accuracy: 0.7010 - val_loss: 1.1170 - val_accuracy: 0.6213\n",
            "Epoch 12/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8226 - accuracy: 0.7120 - val_loss: 1.1830 - val_accuracy: 0.6045\n",
            "Epoch 13/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7969 - accuracy: 0.7201 - val_loss: 1.1570 - val_accuracy: 0.6119\n",
            "Epoch 14/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7689 - accuracy: 0.7300 - val_loss: 1.1394 - val_accuracy: 0.6212\n",
            "Epoch 15/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7404 - accuracy: 0.7396 - val_loss: 1.1402 - val_accuracy: 0.6293\n",
            "Epoch 16/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7217 - accuracy: 0.7459 - val_loss: 1.1806 - val_accuracy: 0.6186\n",
            "Epoch 17/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6933 - accuracy: 0.7552 - val_loss: 1.1965 - val_accuracy: 0.6144\n",
            "Epoch 18/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6776 - accuracy: 0.7598 - val_loss: 1.2316 - val_accuracy: 0.6121\n",
            "Epoch 19/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6491 - accuracy: 0.7702 - val_loss: 1.2366 - val_accuracy: 0.6164\n",
            "Epoch 20/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6292 - accuracy: 0.7770 - val_loss: 1.2862 - val_accuracy: 0.6039\n",
            "Epoch 21/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6085 - accuracy: 0.7828 - val_loss: 1.3009 - val_accuracy: 0.6137\n",
            "Epoch 22/25\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5893 - accuracy: 0.7900 - val_loss: 1.3396 - val_accuracy: 0.6174\n",
            "Epoch 23/25\n",
            "1000/1000 [==============================] - 5s 5ms/step - loss: 0.5686 - accuracy: 0.7980 - val_loss: 1.3226 - val_accuracy: 0.6174\n",
            "Epoch 24/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5484 - accuracy: 0.8051 - val_loss: 1.3459 - val_accuracy: 0.6155\n",
            "Epoch 25/25\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5347 - accuracy: 0.8089 - val_loss: 1.3809 - val_accuracy: 0.6120\n",
            "Epoch 1/25\n",
            "500/500 [==============================] - 3s 5ms/step - loss: 1.7077 - accuracy: 0.3819 - val_loss: 1.5464 - val_accuracy: 0.4507\n",
            "Epoch 2/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.4288 - accuracy: 0.4869 - val_loss: 1.4196 - val_accuracy: 0.4830\n",
            "Epoch 3/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.3178 - accuracy: 0.5285 - val_loss: 1.2840 - val_accuracy: 0.5355\n",
            "Epoch 4/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.2383 - accuracy: 0.5568 - val_loss: 1.2854 - val_accuracy: 0.5447\n",
            "Epoch 5/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.1782 - accuracy: 0.5809 - val_loss: 1.2088 - val_accuracy: 0.5652\n",
            "Epoch 6/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.1251 - accuracy: 0.5980 - val_loss: 1.1846 - val_accuracy: 0.5810\n",
            "Epoch 7/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.0872 - accuracy: 0.6149 - val_loss: 1.1815 - val_accuracy: 0.5852\n",
            "Epoch 8/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.0469 - accuracy: 0.6293 - val_loss: 1.1632 - val_accuracy: 0.5861\n",
            "Epoch 9/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.0099 - accuracy: 0.6427 - val_loss: 1.1347 - val_accuracy: 0.6017\n",
            "Epoch 10/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.9796 - accuracy: 0.6532 - val_loss: 1.1135 - val_accuracy: 0.6103\n",
            "Epoch 11/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.9456 - accuracy: 0.6641 - val_loss: 1.1220 - val_accuracy: 0.6072\n",
            "Epoch 12/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.9194 - accuracy: 0.6763 - val_loss: 1.1370 - val_accuracy: 0.6131\n",
            "Epoch 13/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.8920 - accuracy: 0.6850 - val_loss: 1.1153 - val_accuracy: 0.6219\n",
            "Epoch 14/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.8615 - accuracy: 0.6969 - val_loss: 1.1481 - val_accuracy: 0.6159\n",
            "Epoch 15/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.8412 - accuracy: 0.7016 - val_loss: 1.1268 - val_accuracy: 0.6176\n",
            "Epoch 16/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.8144 - accuracy: 0.7120 - val_loss: 1.1320 - val_accuracy: 0.6220\n",
            "Epoch 17/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.7975 - accuracy: 0.7183 - val_loss: 1.1553 - val_accuracy: 0.6155\n",
            "Epoch 18/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.7734 - accuracy: 0.7259 - val_loss: 1.1510 - val_accuracy: 0.6247\n",
            "Epoch 19/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.7539 - accuracy: 0.7318 - val_loss: 1.1378 - val_accuracy: 0.6262\n",
            "Epoch 20/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.7387 - accuracy: 0.7376 - val_loss: 1.1654 - val_accuracy: 0.6239\n",
            "Epoch 21/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.7125 - accuracy: 0.7486 - val_loss: 1.1843 - val_accuracy: 0.6215\n",
            "Epoch 22/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.6977 - accuracy: 0.7527 - val_loss: 1.2177 - val_accuracy: 0.6225\n",
            "Epoch 23/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.6755 - accuracy: 0.7603 - val_loss: 1.2294 - val_accuracy: 0.6166\n",
            "Epoch 24/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.6624 - accuracy: 0.7649 - val_loss: 1.2516 - val_accuracy: 0.6177\n",
            "Epoch 25/25\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.6502 - accuracy: 0.7665 - val_loss: 1.2574 - val_accuracy: 0.6195\n",
            "Epoch 1/25\n",
            "334/334 [==============================] - 3s 6ms/step - loss: 1.7741 - accuracy: 0.3601 - val_loss: 1.5355 - val_accuracy: 0.4496\n",
            "Epoch 2/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.4467 - accuracy: 0.4845 - val_loss: 1.3668 - val_accuracy: 0.5145\n",
            "Epoch 3/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.3376 - accuracy: 0.5268 - val_loss: 1.3077 - val_accuracy: 0.5303\n",
            "Epoch 4/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.2698 - accuracy: 0.5509 - val_loss: 1.2721 - val_accuracy: 0.5435\n",
            "Epoch 5/25\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.2141 - accuracy: 0.5698 - val_loss: 1.2186 - val_accuracy: 0.5643\n",
            "Epoch 6/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.1688 - accuracy: 0.5857 - val_loss: 1.2435 - val_accuracy: 0.5681\n",
            "Epoch 7/25\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.1271 - accuracy: 0.6040 - val_loss: 1.2177 - val_accuracy: 0.5702\n",
            "Epoch 8/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.0895 - accuracy: 0.6160 - val_loss: 1.1572 - val_accuracy: 0.5894\n",
            "Epoch 9/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.0589 - accuracy: 0.6284 - val_loss: 1.1537 - val_accuracy: 0.5929\n",
            "Epoch 10/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.0254 - accuracy: 0.6400 - val_loss: 1.1485 - val_accuracy: 0.5969\n",
            "Epoch 11/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.9949 - accuracy: 0.6488 - val_loss: 1.1124 - val_accuracy: 0.6138\n",
            "Epoch 12/25\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 0.9637 - accuracy: 0.6607 - val_loss: 1.1300 - val_accuracy: 0.6070\n",
            "Epoch 13/25\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 0.9413 - accuracy: 0.6697 - val_loss: 1.1178 - val_accuracy: 0.6106\n",
            "Epoch 14/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.9177 - accuracy: 0.6757 - val_loss: 1.1319 - val_accuracy: 0.6057\n",
            "Epoch 15/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.9003 - accuracy: 0.6829 - val_loss: 1.1181 - val_accuracy: 0.6150\n",
            "Epoch 16/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.8761 - accuracy: 0.6914 - val_loss: 1.0981 - val_accuracy: 0.6213\n",
            "Epoch 17/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.8642 - accuracy: 0.6949 - val_loss: 1.1211 - val_accuracy: 0.6229\n",
            "Epoch 18/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.8387 - accuracy: 0.7028 - val_loss: 1.1383 - val_accuracy: 0.6071\n",
            "Epoch 19/25\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 0.8233 - accuracy: 0.7089 - val_loss: 1.0995 - val_accuracy: 0.6293\n",
            "Epoch 20/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.8054 - accuracy: 0.7154 - val_loss: 1.1464 - val_accuracy: 0.6162\n",
            "Epoch 21/25\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 0.7903 - accuracy: 0.7219 - val_loss: 1.1425 - val_accuracy: 0.6224\n",
            "Epoch 22/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.7668 - accuracy: 0.7274 - val_loss: 1.1169 - val_accuracy: 0.6251\n",
            "Epoch 23/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.7554 - accuracy: 0.7352 - val_loss: 1.1421 - val_accuracy: 0.6221\n",
            "Epoch 24/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.7417 - accuracy: 0.7392 - val_loss: 1.1365 - val_accuracy: 0.6286\n",
            "Epoch 25/25\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 0.7262 - accuracy: 0.7429 - val_loss: 1.1839 - val_accuracy: 0.6207\n",
            "Epoch 1/25\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 1.7286 - accuracy: 0.3743 - val_loss: 1.4946 - val_accuracy: 0.4690\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 1.4314 - accuracy: 0.4881 - val_loss: 1.3703 - val_accuracy: 0.5152\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 1.3206 - accuracy: 0.5303 - val_loss: 1.3827 - val_accuracy: 0.5048\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 1.2620 - accuracy: 0.5553 - val_loss: 1.2986 - val_accuracy: 0.5355\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.2044 - accuracy: 0.5759 - val_loss: 1.2472 - val_accuracy: 0.5551\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.1622 - accuracy: 0.5901 - val_loss: 1.2278 - val_accuracy: 0.5688\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.1145 - accuracy: 0.6090 - val_loss: 1.1713 - val_accuracy: 0.5860\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.0745 - accuracy: 0.6239 - val_loss: 1.1819 - val_accuracy: 0.5872\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.0441 - accuracy: 0.6333 - val_loss: 1.1403 - val_accuracy: 0.6029\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.0107 - accuracy: 0.6459 - val_loss: 1.1756 - val_accuracy: 0.5881\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.9842 - accuracy: 0.6553 - val_loss: 1.1303 - val_accuracy: 0.6039\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.9562 - accuracy: 0.6641 - val_loss: 1.1221 - val_accuracy: 0.6137\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.9318 - accuracy: 0.6740 - val_loss: 1.1142 - val_accuracy: 0.6142\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.9050 - accuracy: 0.6831 - val_loss: 1.1455 - val_accuracy: 0.6057\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.8852 - accuracy: 0.6901 - val_loss: 1.1485 - val_accuracy: 0.6084\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.8592 - accuracy: 0.6987 - val_loss: 1.1218 - val_accuracy: 0.6099\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.8434 - accuracy: 0.7038 - val_loss: 1.1253 - val_accuracy: 0.6108\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.8241 - accuracy: 0.7115 - val_loss: 1.1095 - val_accuracy: 0.6161\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.8053 - accuracy: 0.7177 - val_loss: 1.1174 - val_accuracy: 0.6200\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.7838 - accuracy: 0.7257 - val_loss: 1.1370 - val_accuracy: 0.6123\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.7671 - accuracy: 0.7324 - val_loss: 1.1292 - val_accuracy: 0.6216\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.7493 - accuracy: 0.7380 - val_loss: 1.1390 - val_accuracy: 0.6265\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.7258 - accuracy: 0.7451 - val_loss: 1.1600 - val_accuracy: 0.6117\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.7126 - accuracy: 0.7492 - val_loss: 1.1799 - val_accuracy: 0.6177\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.7009 - accuracy: 0.7542 - val_loss: 1.1831 - val_accuracy: 0.6188\n"
          ]
        }
      ],
      "source": [
        "# define empty lists to store the accuracy and loss values for model comparison\n",
        "loss = []\n",
        "lossv = []\n",
        "accuracy = []\n",
        "accuracyv = []\n",
        "batch_size = [25, 50, 100, 150, 200]\n",
        "\n",
        "# Training the layers for the CNN using Keras\n",
        "# The three convolutional layers\n",
        "for bs in batch_size:\n",
        "    # Training the layers for the CNN using Keras\n",
        "    # The three convolutional layers\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(tf.keras.layers.Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "    model.add(tf.keras.layers.Conv2D(120, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "    # Flatten the convulational layers and add the dense layers with 84 neurons and the relu activation\n",
        "    # and the output layer with the softmax output layer with 10 nominal output.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    # Fit the training data to the model\n",
        "    NNmodel = model.fit(x_train, y_train, batch_size=bs, epochs=25, validation_data=(x_test, y_test))\n",
        "\n",
        "    accuracyv.append((NNmodel.history['val_accuracy'])[-1])\n",
        "    accuracy.append((NNmodel.history['accuracy'])[-1])\n",
        "    lossv.append((NNmodel.history['val_loss'])[-1])\n",
        "    loss.append((NNmodel.history['loss'])[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "4-Aej5Pf08wV",
        "outputId": "4770d7e9-7751-432e-dbb3-ccbb52f79cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Batch_size against loss plot\n",
        "plt.scatter(batch_size, loss, color ='red') \n",
        "plt.plot(batch_size, loss, color ='red') \n",
        "plt.legend() \n",
        "plt.title(\"Batch_size against loss\", fontsize=15) \n",
        "plt.ylabel('Loss', fontsize = 12) \n",
        "plt.xlabel('Batch_size', fontsize = 12) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "7t1d_i2SYh5E",
        "outputId": "d04968cf-f812-4db8-f7a2-fa23659f19bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Batch_size against loss plot\n",
        "plt.scatter(batch_size, lossv, color ='red') \n",
        "plt.plot(batch_size, lossv, color ='red') \n",
        "plt.legend() \n",
        "plt.title(\"Batch_size against validation loss\", fontsize=15) \n",
        "plt.ylabel('Loss', fontsize = 12) \n",
        "plt.xlabel('Batch_size', fontsize = 12) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "CxWXPvaH08wW",
        "outputId": "dbec6bb9-9088-42fc-a711-26e7cfb1c2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Batch_size against accuracy plot\n",
        "plt.scatter(batch_size, accuracy, color ='blue') \n",
        "plt.plot(batch_size, accuracy, color ='blue') \n",
        "plt.legend() \n",
        "plt.title(\"Batch_sizes against accuracy\", fontsize=15) \n",
        "plt.ylabel('Accuracy', fontsize = 12) \n",
        "plt.xlabel('Batch_size', fontsize = 12) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "z5y2whUAYuwN",
        "outputId": "737b0352-6679-4192-a34f-8a2778e8b1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Batch_size against accuracy plot\n",
        "plt.scatter(batch_size, accuracyv, color ='blue') \n",
        "plt.plot(batch_size, accuracyv, color ='blue') \n",
        "plt.legend() \n",
        "plt.title(\"Batch_sizes against validation accuracy\", fontsize=15) \n",
        "plt.ylabel('Accuracy', fontsize = 12) \n",
        "plt.xlabel('Batch_size', fontsize = 12) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwwC85rs94GP"
      },
      "source": [
        "2. The batch size was also tested against the training loss and accuracy. This was also done for batch sizes of 25, 50, 100, 150, 200 and 500. Increasing the batch size increases the training loss and reduces the training accuracy. however, as a side note, the validation accuracy increasing to a point before dropping again. Considering all of these, I would say the best performing batch size for this particular model would probably be a batch size of 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4_NIwHeEgVT",
        "outputId": "717592a4-b6e4-4c45-fa45-6d8203aa9985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 1.8638 - accuracy: 0.3247 - val_loss: 1.6607 - val_accuracy: 0.4017\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 1.5826 - accuracy: 0.4284 - val_loss: 1.5598 - val_accuracy: 0.4373\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.4853 - accuracy: 0.4676 - val_loss: 1.4534 - val_accuracy: 0.4742\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.4278 - accuracy: 0.4894 - val_loss: 1.4132 - val_accuracy: 0.4914\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.3830 - accuracy: 0.5070 - val_loss: 1.3801 - val_accuracy: 0.5048\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.3469 - accuracy: 0.5188 - val_loss: 1.3496 - val_accuracy: 0.5183\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.3154 - accuracy: 0.5325 - val_loss: 1.3308 - val_accuracy: 0.5218\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.2874 - accuracy: 0.5425 - val_loss: 1.3112 - val_accuracy: 0.5276\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.2641 - accuracy: 0.5517 - val_loss: 1.2887 - val_accuracy: 0.5450\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.2405 - accuracy: 0.5633 - val_loss: 1.2726 - val_accuracy: 0.5471\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.2726 - accuracy: 0.5471\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 5s 4ms/step - loss: 1.9395 - accuracy: 0.2965 - val_loss: 1.6895 - val_accuracy: 0.3945\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.6196 - accuracy: 0.4201 - val_loss: 1.5656 - val_accuracy: 0.4419\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.5226 - accuracy: 0.4539 - val_loss: 1.5040 - val_accuracy: 0.4595\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4574 - accuracy: 0.4796 - val_loss: 1.4577 - val_accuracy: 0.4789\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4130 - accuracy: 0.4959 - val_loss: 1.4152 - val_accuracy: 0.4931\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3780 - accuracy: 0.5061 - val_loss: 1.3820 - val_accuracy: 0.5051\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3480 - accuracy: 0.5182 - val_loss: 1.3571 - val_accuracy: 0.5176\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3225 - accuracy: 0.5291 - val_loss: 1.3430 - val_accuracy: 0.5212\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2989 - accuracy: 0.5379 - val_loss: 1.3355 - val_accuracy: 0.5270\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2794 - accuracy: 0.5444 - val_loss: 1.3029 - val_accuracy: 0.5422\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.3029 - accuracy: 0.5422\n",
            "Epoch 1/10\n",
            "500/500 [==============================] - 3s 5ms/step - loss: 1.9785 - accuracy: 0.2781 - val_loss: 1.7489 - val_accuracy: 0.3738\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.6795 - accuracy: 0.3961 - val_loss: 1.6542 - val_accuracy: 0.3970\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.5939 - accuracy: 0.4275 - val_loss: 1.5641 - val_accuracy: 0.4370\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.5404 - accuracy: 0.4468 - val_loss: 1.5234 - val_accuracy: 0.4552\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.5041 - accuracy: 0.4597 - val_loss: 1.5028 - val_accuracy: 0.4568\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.4728 - accuracy: 0.4719 - val_loss: 1.4661 - val_accuracy: 0.4740\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.4474 - accuracy: 0.4821 - val_loss: 1.4403 - val_accuracy: 0.4845\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.4256 - accuracy: 0.4928 - val_loss: 1.4377 - val_accuracy: 0.4820\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.4066 - accuracy: 0.4975 - val_loss: 1.4307 - val_accuracy: 0.4898\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 1.3881 - accuracy: 0.5064 - val_loss: 1.3949 - val_accuracy: 0.4992\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.3949 - accuracy: 0.4992\n",
            "Epoch 1/10\n",
            "334/334 [==============================] - 3s 7ms/step - loss: 2.0866 - accuracy: 0.2400 - val_loss: 1.8433 - val_accuracy: 0.3410\n",
            "Epoch 2/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.7498 - accuracy: 0.3751 - val_loss: 1.6904 - val_accuracy: 0.3907\n",
            "Epoch 3/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.6311 - accuracy: 0.4181 - val_loss: 1.5920 - val_accuracy: 0.4316\n",
            "Epoch 4/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.5653 - accuracy: 0.4409 - val_loss: 1.5437 - val_accuracy: 0.4449\n",
            "Epoch 5/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.5151 - accuracy: 0.4583 - val_loss: 1.5060 - val_accuracy: 0.4627\n",
            "Epoch 6/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.4752 - accuracy: 0.4741 - val_loss: 1.4690 - val_accuracy: 0.4682\n",
            "Epoch 7/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.4433 - accuracy: 0.4847 - val_loss: 1.4418 - val_accuracy: 0.4841\n",
            "Epoch 8/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.4154 - accuracy: 0.4944 - val_loss: 1.4290 - val_accuracy: 0.4880\n",
            "Epoch 9/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.3915 - accuracy: 0.5048 - val_loss: 1.3956 - val_accuracy: 0.5028\n",
            "Epoch 10/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.3700 - accuracy: 0.5115 - val_loss: 1.3925 - val_accuracy: 0.5012\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.3925 - accuracy: 0.5012\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 2.1760 - accuracy: 0.2026 - val_loss: 2.0122 - val_accuracy: 0.2750\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.9338 - accuracy: 0.3030 - val_loss: 1.8537 - val_accuracy: 0.3355\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.7967 - accuracy: 0.3567 - val_loss: 1.7515 - val_accuracy: 0.3750\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.7088 - accuracy: 0.3881 - val_loss: 1.6906 - val_accuracy: 0.4003\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 1.6429 - accuracy: 0.4135 - val_loss: 1.6098 - val_accuracy: 0.4290\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.5895 - accuracy: 0.4331 - val_loss: 1.5822 - val_accuracy: 0.4324\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.5537 - accuracy: 0.4439 - val_loss: 1.5510 - val_accuracy: 0.4398\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.5244 - accuracy: 0.4547 - val_loss: 1.5197 - val_accuracy: 0.4559\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4991 - accuracy: 0.4631 - val_loss: 1.5036 - val_accuracy: 0.4617\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4804 - accuracy: 0.4712 - val_loss: 1.4855 - val_accuracy: 0.4699\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.4855 - accuracy: 0.4699\n",
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 1.5900 - accuracy: 0.4202 - val_loss: 1.4108 - val_accuracy: 0.4859\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.3144 - accuracy: 0.5289 - val_loss: 1.2701 - val_accuracy: 0.5458\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.1986 - accuracy: 0.5749 - val_loss: 1.2248 - val_accuracy: 0.5638\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.1240 - accuracy: 0.6018 - val_loss: 1.1641 - val_accuracy: 0.5859\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1.0624 - accuracy: 0.6237 - val_loss: 1.1319 - val_accuracy: 0.6018\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1.0114 - accuracy: 0.6435 - val_loss: 1.1488 - val_accuracy: 0.5924\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.9692 - accuracy: 0.6553 - val_loss: 1.1440 - val_accuracy: 0.5971\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.9278 - accuracy: 0.6702 - val_loss: 1.1271 - val_accuracy: 0.6021\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.8904 - accuracy: 0.6826 - val_loss: 1.2084 - val_accuracy: 0.5929\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 0.8522 - accuracy: 0.6967 - val_loss: 1.1810 - val_accuracy: 0.6015\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 1.1810 - accuracy: 0.6015\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 5s 4ms/step - loss: 1.6024 - accuracy: 0.4148 - val_loss: 1.4415 - val_accuracy: 0.4679\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3196 - accuracy: 0.5263 - val_loss: 1.2712 - val_accuracy: 0.5474\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2060 - accuracy: 0.5728 - val_loss: 1.2242 - val_accuracy: 0.5672\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1263 - accuracy: 0.6017 - val_loss: 1.1856 - val_accuracy: 0.5765\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0633 - accuracy: 0.6251 - val_loss: 1.1232 - val_accuracy: 0.6043\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0094 - accuracy: 0.6437 - val_loss: 1.1016 - val_accuracy: 0.6151\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9610 - accuracy: 0.6610 - val_loss: 1.1078 - val_accuracy: 0.6067\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9220 - accuracy: 0.6752 - val_loss: 1.0910 - val_accuracy: 0.6193\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8892 - accuracy: 0.6870 - val_loss: 1.1565 - val_accuracy: 0.6063\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8509 - accuracy: 0.7012 - val_loss: 1.0885 - val_accuracy: 0.6245\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.0885 - accuracy: 0.6245\n",
            "Epoch 1/10\n",
            "500/500 [==============================] - 3s 5ms/step - loss: 1.6293 - accuracy: 0.4042 - val_loss: 1.4153 - val_accuracy: 0.4861\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.3632 - accuracy: 0.5106 - val_loss: 1.3064 - val_accuracy: 0.5248\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.2620 - accuracy: 0.5503 - val_loss: 1.3064 - val_accuracy: 0.5394\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.1926 - accuracy: 0.5766 - val_loss: 1.2246 - val_accuracy: 0.5652\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.1336 - accuracy: 0.5995 - val_loss: 1.2046 - val_accuracy: 0.5719\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.0854 - accuracy: 0.6161 - val_loss: 1.1626 - val_accuracy: 0.5879\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.0485 - accuracy: 0.6302 - val_loss: 1.1577 - val_accuracy: 0.5930\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 1.0049 - accuracy: 0.6441 - val_loss: 1.1242 - val_accuracy: 0.6061\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.9740 - accuracy: 0.6578 - val_loss: 1.1737 - val_accuracy: 0.5935\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 0.9474 - accuracy: 0.6678 - val_loss: 1.1161 - val_accuracy: 0.6112\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1161 - accuracy: 0.6112\n",
            "Epoch 1/10\n",
            "334/334 [==============================] - 3s 6ms/step - loss: 1.7507 - accuracy: 0.3669 - val_loss: 1.5177 - val_accuracy: 0.4562\n",
            "Epoch 2/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.4508 - accuracy: 0.4762 - val_loss: 1.3818 - val_accuracy: 0.5045\n",
            "Epoch 3/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.3488 - accuracy: 0.5166 - val_loss: 1.3478 - val_accuracy: 0.5184\n",
            "Epoch 4/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.2757 - accuracy: 0.5461 - val_loss: 1.2557 - val_accuracy: 0.5548\n",
            "Epoch 5/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.2107 - accuracy: 0.5712 - val_loss: 1.2349 - val_accuracy: 0.5633\n",
            "Epoch 6/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.1604 - accuracy: 0.5911 - val_loss: 1.2247 - val_accuracy: 0.5701\n",
            "Epoch 7/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.1132 - accuracy: 0.6083 - val_loss: 1.2022 - val_accuracy: 0.5769\n",
            "Epoch 8/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.0833 - accuracy: 0.6188 - val_loss: 1.1841 - val_accuracy: 0.5862\n",
            "Epoch 9/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.0442 - accuracy: 0.6316 - val_loss: 1.1788 - val_accuracy: 0.5883\n",
            "Epoch 10/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.0141 - accuracy: 0.6429 - val_loss: 1.1879 - val_accuracy: 0.5860\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1879 - accuracy: 0.5860\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 1.7726 - accuracy: 0.3575 - val_loss: 1.5533 - val_accuracy: 0.4333\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4584 - accuracy: 0.4734 - val_loss: 1.4325 - val_accuracy: 0.4753\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.3514 - accuracy: 0.5156 - val_loss: 1.3150 - val_accuracy: 0.5280\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 1.2871 - accuracy: 0.5400 - val_loss: 1.3000 - val_accuracy: 0.5397\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.2352 - accuracy: 0.5590 - val_loss: 1.2607 - val_accuracy: 0.5495\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.1889 - accuracy: 0.5751 - val_loss: 1.2292 - val_accuracy: 0.5623\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.1551 - accuracy: 0.5907 - val_loss: 1.2090 - val_accuracy: 0.5680\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.1170 - accuracy: 0.6035 - val_loss: 1.1991 - val_accuracy: 0.5795\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.0854 - accuracy: 0.6168 - val_loss: 1.1903 - val_accuracy: 0.5839\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.0470 - accuracy: 0.6318 - val_loss: 1.1778 - val_accuracy: 0.5852\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.1778 - accuracy: 0.5852\n",
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.1008 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.1010 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.1016 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.0996 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.1013 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.1003 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3043 - accuracy: 0.0982 - val_loss: 2.3045 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3043 - accuracy: 0.0991 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3044 - accuracy: 0.0981 - val_loss: 2.3049 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3042 - accuracy: 0.1006 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3035 - accuracy: 0.1000\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 5s 4ms/step - loss: 1.8283 - accuracy: 0.3220 - val_loss: 1.6899 - val_accuracy: 0.3891\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.6134 - accuracy: 0.4157 - val_loss: 1.5951 - val_accuracy: 0.4206\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.5747 - accuracy: 0.4362 - val_loss: 1.6138 - val_accuracy: 0.4181\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.5186 - accuracy: 0.4578 - val_loss: 1.5912 - val_accuracy: 0.4427\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4984 - accuracy: 0.4691 - val_loss: 1.5471 - val_accuracy: 0.4519\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4657 - accuracy: 0.4807 - val_loss: 1.5187 - val_accuracy: 0.4666\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4544 - accuracy: 0.4855 - val_loss: 1.5085 - val_accuracy: 0.4710\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4402 - accuracy: 0.4968 - val_loss: 1.4770 - val_accuracy: 0.4838\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4404 - accuracy: 0.4964 - val_loss: 1.4966 - val_accuracy: 0.4844\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4220 - accuracy: 0.5023 - val_loss: 1.4967 - val_accuracy: 0.4774\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.4967 - accuracy: 0.4774\n",
            "Epoch 1/10\n",
            "500/500 [==============================] - 3s 6ms/step - loss: 2.3054 - accuracy: 0.0983 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3034 - accuracy: 0.0996 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3035 - accuracy: 0.0995 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3035 - accuracy: 0.0988 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3036 - accuracy: 0.0983 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3034 - accuracy: 0.0986 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3035 - accuracy: 0.0973 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3034 - accuracy: 0.0976 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3035 - accuracy: 0.0984 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3035 - accuracy: 0.0994 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3031 - accuracy: 0.1000\n",
            "Epoch 1/10\n",
            "334/334 [==============================] - 3s 6ms/step - loss: 1.8859 - accuracy: 0.3058 - val_loss: 1.7387 - val_accuracy: 0.3595\n",
            "Epoch 2/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.6822 - accuracy: 0.3877 - val_loss: 1.6729 - val_accuracy: 0.3843\n",
            "Epoch 3/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.6359 - accuracy: 0.4035 - val_loss: 1.6478 - val_accuracy: 0.3944\n",
            "Epoch 4/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.6211 - accuracy: 0.4112 - val_loss: 1.6370 - val_accuracy: 0.4055\n",
            "Epoch 5/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.6098 - accuracy: 0.4167 - val_loss: 1.6096 - val_accuracy: 0.4087\n",
            "Epoch 6/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.5866 - accuracy: 0.4241 - val_loss: 1.5946 - val_accuracy: 0.4151\n",
            "Epoch 7/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.5770 - accuracy: 0.4274 - val_loss: 1.6398 - val_accuracy: 0.4039\n",
            "Epoch 8/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.5685 - accuracy: 0.4297 - val_loss: 1.5871 - val_accuracy: 0.4289\n",
            "Epoch 9/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 1.5626 - accuracy: 0.4320 - val_loss: 1.6083 - val_accuracy: 0.4106\n",
            "Epoch 10/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 1.5511 - accuracy: 0.4365 - val_loss: 1.6087 - val_accuracy: 0.4255\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.6087 - accuracy: 0.4255\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 1.9299 - accuracy: 0.2817 - val_loss: 1.7170 - val_accuracy: 0.3646\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.6833 - accuracy: 0.3729 - val_loss: 1.6414 - val_accuracy: 0.3920\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.6068 - accuracy: 0.4037 - val_loss: 1.6271 - val_accuracy: 0.4029\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.5381 - accuracy: 0.4354 - val_loss: 1.5354 - val_accuracy: 0.4376\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4818 - accuracy: 0.4586 - val_loss: 1.4893 - val_accuracy: 0.4560\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4544 - accuracy: 0.4701 - val_loss: 1.4980 - val_accuracy: 0.4592\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.4147 - accuracy: 0.4864 - val_loss: 1.5178 - val_accuracy: 0.4578\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.3888 - accuracy: 0.4982 - val_loss: 1.4598 - val_accuracy: 0.4716\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.3381 - accuracy: 0.5167 - val_loss: 1.4732 - val_accuracy: 0.4747\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 1.3394 - accuracy: 0.5153 - val_loss: 1.4403 - val_accuracy: 0.4804\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.4403 - accuracy: 0.4804\n",
            "Epoch 1/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.7693 - accuracy: 0.0985 - val_loss: 2.3041 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3169 - accuracy: 0.1008 - val_loss: 2.3152 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3171 - accuracy: 0.1003 - val_loss: 2.3255 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3178 - accuracy: 0.1016 - val_loss: 2.3207 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3181 - accuracy: 0.0995 - val_loss: 2.3259 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3170 - accuracy: 0.1010 - val_loss: 2.3207 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3176 - accuracy: 0.0990 - val_loss: 2.3150 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3171 - accuracy: 0.1002 - val_loss: 2.3162 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3176 - accuracy: 0.1003 - val_loss: 2.3150 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 2.3164 - accuracy: 0.1013 - val_loss: 2.3134 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3134 - accuracy: 0.1000\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 5s 4ms/step - loss: 2.4471 - accuracy: 0.0993 - val_loss: 2.3113 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3127 - accuracy: 0.0984 - val_loss: 2.3098 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3129 - accuracy: 0.1004 - val_loss: 2.3290 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3128 - accuracy: 0.0976 - val_loss: 2.3114 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3137 - accuracy: 0.1004 - val_loss: 2.3259 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3126 - accuracy: 0.1031 - val_loss: 2.3189 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3123 - accuracy: 0.0999 - val_loss: 2.3085 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3125 - accuracy: 0.1010 - val_loss: 2.3214 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3113 - accuracy: 0.1021 - val_loss: 2.3140 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3139 - accuracy: 0.0969 - val_loss: 2.3149 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3149 - accuracy: 0.1000\n",
            "Epoch 1/10\n",
            "500/500 [==============================] - 3s 5ms/step - loss: 3.2379 - accuracy: 0.0991 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3087 - accuracy: 0.0984 - val_loss: 2.3127 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 2s 4ms/step - loss: 2.3089 - accuracy: 0.1010 - val_loss: 2.3101 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3093 - accuracy: 0.1010 - val_loss: 2.3092 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3099 - accuracy: 0.1000 - val_loss: 2.3109 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3097 - accuracy: 0.1001 - val_loss: 2.3082 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3092 - accuracy: 0.0998 - val_loss: 2.3085 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3098 - accuracy: 0.0989 - val_loss: 2.3156 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3111 - accuracy: 0.0979 - val_loss: 2.3094 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 2s 5ms/step - loss: 2.3099 - accuracy: 0.1017 - val_loss: 2.3093 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3093 - accuracy: 0.1000\n",
            "Epoch 1/10\n",
            "334/334 [==============================] - 4s 10ms/step - loss: 5.4728 - accuracy: 0.1002 - val_loss: 2.3079 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "334/334 [==============================] - 3s 7ms/step - loss: 2.3074 - accuracy: 0.1008 - val_loss: 2.3080 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 2.3075 - accuracy: 0.1003 - val_loss: 2.3077 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 2.3084 - accuracy: 0.1023 - val_loss: 2.3075 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 2.3083 - accuracy: 0.1009 - val_loss: 2.3061 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 2.3078 - accuracy: 0.0974 - val_loss: 2.3100 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 2.3089 - accuracy: 0.1020 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 2.3084 - accuracy: 0.0987 - val_loss: 2.3101 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "334/334 [==============================] - 2s 6ms/step - loss: 2.3080 - accuracy: 0.0987 - val_loss: 2.3061 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "334/334 [==============================] - 2s 5ms/step - loss: 2.3086 - accuracy: 0.1004 - val_loss: 2.3061 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3061 - accuracy: 0.1000\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 2.3335 - accuracy: 0.1007 - val_loss: 2.3106 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 2.3071 - accuracy: 0.1019 - val_loss: 2.3121 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 2.3079 - accuracy: 0.0985 - val_loss: 2.3074 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 2.3078 - accuracy: 0.1017 - val_loss: 2.3067 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 2.3074 - accuracy: 0.1004 - val_loss: 2.3058 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 2.3069 - accuracy: 0.0988 - val_loss: 2.3067 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 2.3078 - accuracy: 0.0989 - val_loss: 2.3044 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 2.3073 - accuracy: 0.1013 - val_loss: 2.3049 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 2.3071 - accuracy: 0.0976 - val_loss: 2.3066 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 2.3086 - accuracy: 0.1001 - val_loss: 2.3067 - val_accuracy: 0.1000\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3067 - accuracy: 0.1000\n"
          ]
        }
      ],
      "source": [
        "a = []\n",
        "d = []\n",
        "e = []\n",
        "\n",
        "# define the model for the convolutional neural net for reducing filters\n",
        "for lr in learning_rate:\n",
        "    for bs in batch_size:\n",
        "        # Training the layers for the CNN using Keras\n",
        "        # The three convolutional layers\n",
        "\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.keras.layers.Conv2D(6, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
        "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "        model.add(tf.keras.layers.Conv2D(16, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "        model.add(tf.keras.layers.Conv2D(120, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "        # Flatten the convulational layers and add the dense layers with 84 neurons and the relu activation\n",
        "        # and the output layer with the softmax output layer with 10 nominal output.\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(84, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "        # Compile\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = lr)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "        # Fit the training data to the model\n",
        "        NNmodel = model.fit(x_train, y_train, batch_size=bs, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "        # lists\n",
        "        a.append(lr)\n",
        "        d.append(bs)\n",
        "        e.append(model.evaluate(x_test, y_test)[1])\n",
        "\n",
        "models4 = pd.DataFrame({'Learning Rate': a,\n",
        "                        'Batch Size': d,\n",
        "                        'Accuracy': e})\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "lTQTjRpJ30CI",
        "outputId": "3c5333bf-43b4-4efb-eca7-d2435b931740"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Learning Rate  Batch Size  Accuracy\n",
              "6          0.0010          50    0.6245\n",
              "7          0.0010         100    0.6112\n",
              "5          0.0010          25    0.6015\n",
              "8          0.0010         150    0.5860\n",
              "9          0.0010         200    0.5852\n",
              "0          0.0001          25    0.5471\n",
              "1          0.0001          50    0.5422\n",
              "3          0.0001         150    0.5012\n",
              "2          0.0001         100    0.4992\n",
              "14         0.0100         200    0.4804\n",
              "11         0.0100          50    0.4774\n",
              "4          0.0001         200    0.4699\n",
              "13         0.0100         150    0.4255\n",
              "16         0.1000          50    0.1000\n",
              "18         0.1000         150    0.1000\n",
              "17         0.1000         100    0.1000\n",
              "10         0.0100          25    0.1000\n",
              "15         0.1000          25    0.1000\n",
              "12         0.0100         100    0.1000\n",
              "19         0.1000         200    0.1000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73072c7a-c2b3-4385-803a-30d7658203c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0010</td>\n",
              "      <td>50</td>\n",
              "      <td>0.6245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0010</td>\n",
              "      <td>100</td>\n",
              "      <td>0.6112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0010</td>\n",
              "      <td>25</td>\n",
              "      <td>0.6015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0010</td>\n",
              "      <td>150</td>\n",
              "      <td>0.5860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0010</td>\n",
              "      <td>200</td>\n",
              "      <td>0.5852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>25</td>\n",
              "      <td>0.5471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>50</td>\n",
              "      <td>0.5422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>150</td>\n",
              "      <td>0.5012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>100</td>\n",
              "      <td>0.4992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>200</td>\n",
              "      <td>0.4804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>50</td>\n",
              "      <td>0.4774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0001</td>\n",
              "      <td>200</td>\n",
              "      <td>0.4699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>150</td>\n",
              "      <td>0.4255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.1000</td>\n",
              "      <td>50</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.1000</td>\n",
              "      <td>150</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.1000</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>25</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.1000</td>\n",
              "      <td>25</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.1000</td>\n",
              "      <td>200</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73072c7a-c2b3-4385-803a-30d7658203c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-73072c7a-c2b3-4385-803a-30d7658203c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-73072c7a-c2b3-4385-803a-30d7658203c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "models4.sort_values('Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QADV6QuFBtc5"
      },
      "source": [
        "3. Using the different parameters in the model to obtain the best accuracy would be employing the best batch size and best learning rate as found. This is with a batch size of 50 and a learning rate of 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpJGGWXCHjrO",
        "outputId": "a7a84524-66d0-4877-e548-0d188774650a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_110\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_220 (Dense)           (None, 32, 32, 6)         24        \n",
            "                                                                 \n",
            " dense_221 (Dense)           (None, 32, 32, 16)        112       \n",
            "                                                                 \n",
            " dense_222 (Dense)           (None, 32, 32, 120)       2040      \n",
            "                                                                 \n",
            " flatten_110 (Flatten)       (None, 122880)            0         \n",
            "                                                                 \n",
            " dense_223 (Dense)           (None, 84)                10322004  \n",
            "                                                                 \n",
            " dense_224 (Dense)           (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,325,030\n",
            "Trainable params: 10,325,030\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "100/100 [==============================] - 5s 44ms/step - loss: 2.5875 - accuracy: 0.1230\n",
            "Epoch 2/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 2.2139 - accuracy: 0.1555\n",
            "Epoch 3/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 2.1323 - accuracy: 0.2022\n",
            "Epoch 4/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 2.0483 - accuracy: 0.2462\n",
            "Epoch 5/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.9820 - accuracy: 0.2849\n",
            "Epoch 6/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.9227 - accuracy: 0.3021\n",
            "Epoch 7/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.8696 - accuracy: 0.3190\n",
            "Epoch 8/25\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 1.8307 - accuracy: 0.3304\n",
            "Epoch 9/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.7944 - accuracy: 0.3397\n",
            "Epoch 10/25\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 1.7654 - accuracy: 0.3495\n",
            "Epoch 11/25\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 1.7415 - accuracy: 0.3590\n",
            "Epoch 12/25\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 1.7189 - accuracy: 0.3671\n",
            "Epoch 13/25\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 1.6974 - accuracy: 0.3761\n",
            "Epoch 14/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.6816 - accuracy: 0.3793\n",
            "Epoch 15/25\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 1.6671 - accuracy: 0.3845\n",
            "Epoch 16/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.6507 - accuracy: 0.3925\n",
            "Epoch 17/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.6448 - accuracy: 0.3926\n",
            "Epoch 18/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.6280 - accuracy: 0.3974\n",
            "Epoch 19/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.6173 - accuracy: 0.4009\n",
            "Epoch 20/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.6092 - accuracy: 0.4037\n",
            "Epoch 21/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.5983 - accuracy: 0.4062\n",
            "Epoch 22/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.5942 - accuracy: 0.4076\n",
            "Epoch 23/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.5849 - accuracy: 0.4106\n",
            "Epoch 24/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.5717 - accuracy: 0.4146\n",
            "Epoch 25/25\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.5654 - accuracy: 0.4161\n"
          ]
        }
      ],
      "source": [
        "# feed forward neural network\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Input(shape=(32, 32, 3), name='Input-Layer'))\n",
        "model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(120, activation='relu'))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(84, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "model.build(input_shape=(32, 32, 3))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "NNmodel = model.fit(x_train, y_train, batch_size=500, epochs=25, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_xrLFsEw365H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55077420-ec0d-4c3d-9b63-9a1bde852cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 1.6648 - accuracy: 0.3905\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.390500009059906"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# evaluate model\n",
        "model.evaluate(x_test, y_test)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMSXcEDDPrAX"
      },
      "source": [
        "a. The performance had an accuracy of 0.3905"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLTOl-BDPwzD"
      },
      "source": [
        "b. The parameters involved in the network model were 10,325,030."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tyG1fG908wW"
      },
      "source": [
        "# Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIoAko94fF-"
      },
      "source": [
        "For the question 3, I solved it by hand and attached it here under this particular text. however, for some reason, it is not showing here in the colab. I will add a markdown file in my github repository and link it in the submission as a comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUuiC0FrNA52"
      },
      "source": [
        "![](20221024_110928.jpg)\n",
        "![](20221024_111000.jpg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c536098342e249770475e76571c08326ac21f990e5ca158aeaf10eded2866f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}